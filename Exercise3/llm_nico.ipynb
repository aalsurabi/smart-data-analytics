{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "683a90f6-4a88-40d4-8f80-2acdc1edace3",
   "metadata": {},
   "source": [
    "# 0. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a9357a2-e2b0-4536-b3ac-11bc26d8cb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting trl\n",
      "  Downloading trl-0.9.4-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface_hub) (3.13.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub) (4.11.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.5.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m326.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from trl) (2.2.2+cu121)\n",
      "Collecting accelerate (from trl)\n",
      "  Downloading accelerate-0.31.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting datasets (from trl)\n",
      "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting tyro>=0.5.11 (from trl)\n",
      "  Downloading tyro-0.8.4-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.4.0->trl) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.4.0->trl) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.4.0->trl) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.4.0->trl) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.4.0->trl) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.4.0->trl) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.11/site-packages (from torch>=1.4.0->trl) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.4.0->trl) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch>=1.4.0->trl) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.4.0->trl) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch>=1.4.0->trl) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.4.0->trl) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.11/site-packages (from torch>=1.4.0->trl) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.4.0->trl) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.4.0->trl) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4.0->trl) (12.4.127)\n",
      "Collecting docstring-parser>=0.14.1 (from tyro>=0.5.11->trl)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.11/site-packages (from tyro>=0.5.11->trl) (13.7.1)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n",
      "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate->trl) (5.9.8)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets->trl) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.11/site-packages (from datasets->trl) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets->trl) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets->trl) (2.2.1)\n",
      "Collecting requests (from huggingface_hub)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.42.1 (from huggingface_hub)\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m376.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash (from datasets->trl)\n",
      "  Downloading xxhash-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets->trl)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets->trl) (3.9.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub) (2024.2.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->trl) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->trl) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->trl) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->trl) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->trl) (1.9.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.17.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.4.0->trl) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets->trl) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets->trl) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets->trl) (2024.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\n",
      "Downloading huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.6/402.6 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m155.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.9.4-py3-none-any.whl (226 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m467.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.5.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (785 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m785.0/785.0 kB\u001b[0m \u001b[31m108.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m122.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.8.4-py3-none-any.whl (102 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/102.4 kB\u001b[0m \u001b[31m419.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m229.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m185.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m399.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m422.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m472.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m539.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece, xxhash, tqdm, shtab, safetensors, requests, regex, multiprocess, docstring-parser, huggingface_hub, tyro, tokenizers, transformers, datasets, accelerate, trl\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.2\n",
      "    Uninstalling tqdm-4.66.2:\n",
      "      Successfully uninstalled tqdm-4.66.2\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "Successfully installed accelerate-0.31.0 datasets-2.20.0 docstring-parser-0.16 huggingface_hub-0.23.4 multiprocess-0.70.16 regex-2024.5.15 requests-2.32.3 safetensors-0.4.3 sentencepiece-0.2.0 shtab-1.7.1 tokenizers-0.19.1 tqdm-4.66.4 transformers-4.41.2 trl-0.9.4 tyro-0.8.4 xxhash-3.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub transformers sentencepiece trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d4431b4-70e4-442a-9a2f-f0eea165420f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-20 13:42:19.724246: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-20 13:42:19.765314: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-20 13:42:20.521885: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import RobertaTokenizer, T5ForConditionalGeneration\n",
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "from transformers import pipeline\n",
    "from trl import setup_chat_format,SFTTrainer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc177a33-b22d-40f9-b4b4-032b0d801605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.2+cu121'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da69a2a4-ba9a-4344-a4aa-eba87208b5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/jovyan/.cache/huggingface/token\n",
      "Login successful\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Call the login function to authenticate. You'll need to enter your credentials or token.\n",
    "token = \"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\"\n",
    "login(token=token)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e962cb-1051-4a48-8863-ce6c771faabc",
   "metadata": {},
   "source": [
    "# 1. Model trys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19338a3d-47b1-40a8-886f-d788ec581ac0",
   "metadata": {},
   "source": [
    "We tried using Meta-Llama 8B, but it was too heavy, causing the kernel to shut down. The same issue occurred with Mistal; the model is too large.\n",
    "\n",
    "Tutorial from huggingface => https://huggingface.co/docs/transformers/llm_tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e637b82b-7310-48b1-972e-d3c5515a3e50",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1.1 Meta-LLama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b329061-c742-4dc5-9c18-ae972a660c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the model name\n",
    "# model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "# # Load the tokenizer and model with token\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, token=token).to(device)\n",
    "\n",
    "# # Define the prompt\n",
    "# prompt = \"\"\"\n",
    "# Generate a Playwright script in TypeScript that does the following:\n",
    "# 1. Navigate to eBay Kleinanzeigen\n",
    "# 2. Accept cookies\n",
    "# 3. Accept the GDPR banner\n",
    "# 4. Search for a phone\n",
    "# 5. Take a screenshot of the search results\n",
    "# \"\"\"\n",
    "\n",
    "# # Tokenize the input\n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# # Generate the script\n",
    "# outputs = model.generate(inputs.input_ids, max_length=500, num_return_sequences=1)\n",
    "\n",
    "# # Decode and print the generated text\n",
    "# generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb23d2c7-c430-4c35-a0fd-7b76f78f9bcb",
   "metadata": {},
   "source": [
    "## 1.2 GPT-Neo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1ea5c15-206e-413e-aa2a-c8c9b0013494",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"EleutherAI/gpt-neo-1.3B\" # Model name for GPT-1.3\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPTNeoForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8823e729-1c6f-4f41-885b-69285f299b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  198,  8645,   378,   257, 11361,  4226,  1262,   262,  3811, 29995,\n",
      "          5888,   284,  1620, 12454,  4856,   319,  7444,    13,   220,   198,\n",
      "           464,  4226,   815,    25,   198,    16,    13, 13244, 10055,   284,\n",
      "           262,  7444, 35699,    13,   198,    17,    13, 21699, 14746,    13,\n",
      "           198,    18,    13, 11140,   329,   257,  2008,    13,   198,    19,\n",
      "            13,  7214,   257, 22032,   286,   262,  2989,  2482,    13,   198,\n",
      "           464,  2163,   815,   923,   355,  5679,    25,   198,  4299,  1332,\n",
      "            62, 11604,     7,  7700,    25,  7873,  2599,   198]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Define the prompt\n",
    "prompt = \"\"\"\n",
    "Generate a Python script using the Playwright library to perform UI testing on YouTube. \n",
    "The script should:\n",
    "1. Navigate to the YouTube webpage.\n",
    "2. Accept cookies.\n",
    "3. Search for a video.\n",
    "4. Take a screenshot of the search results.\n",
    "The function should start as follows:\n",
    "def test_youtube(page: Page):\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize input\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e79eda90-4aaf-40de-b591-21472c6b580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "outputs = model.generate(input_ids=input_ids, \n",
    "                         pad_token_id=tokenizer.eos_token_id,  # Set pad token ID to EOS token ID\n",
    "                         attention_mask=input_ids.new_ones(input_ids.shape).to(device),  # Create attention mask\n",
    "                         max_length=500,\n",
    "                         do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95920567-1eae-4f0a-8c92-aab75968fd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generate a Python script using the Playwright library to perform UI testing on YouTube. \n",
      "The script should:\n",
      "1. Navigate to the YouTube webpage.\n",
      "2. Accept cookies.\n",
      "3. Search for a video.\n",
      "4. Take a screenshot of the search results.\n",
      "The function should start as follows:\n",
      "def test_youtube(page: Page):\n",
      "    video_url = page.url\n",
      "    response = browser.get(video_url, params={'no_result' : True})\\\n",
      "    page\\\n",
      "     -> wait_for_page_to_load(video_url)\\\n",
      "     -> assert response(video_url).text\n",
      "\n",
      "I have a problem with the wait_for_page_to_load in the first line (line \\), because I always create the page before performing the test and the page isn't always available. I have tried to put it inside \\ if it was always available: but it is not.\n",
      "So, I need to wait for the video to appear, but I can't wait for the page to be ready. Can you please help me?\n",
      "\n",
      "A:\n",
      "\n",
      "The correct wait\\_for_page_to_load function to use is one you wrote (without the () on line \\).\n",
      "\n",
      "wait_for_page_to_load(page) - waits until page will be updated with status True or False. This is an absolute requirement.\n",
      "\n",
      "import requests\n",
      "from playwright.tools import wait_until, create_page as pagecreate\n",
      "wait_until(page.open)\n",
      "page.open = True\n",
      "\n",
      "However, because you don't specify if there is content waiting to be displayed or not, that will still be true regardless of the fact that a page is found\n",
      "\n",
      "A:\n",
      "\n",
      "Your code needs to wait until it can find the Youtube page at the URL you give.  If you only give it a string, it will still attempt to find it but will fail.\n",
      "For example, the following will not find your page:\n",
      "try:\n",
      "    response = browser.get(r'https://www.youtube.com/watch?v=CiOq_ZsXwEo')\n",
      "except urlopen:\n",
      "    continue\n",
      "\n",
      "Instead you have to give it a list of the pages\n"
     ]
    }
   ],
   "source": [
    "# Decode and print the generated text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27c1fd4-1bbb-4143-bdd4-d9e06ce58a0e",
   "metadata": {},
   "source": [
    "It do something, but i dont think it can the code, seems wrong. But with finetuning the model ca do better. Also a lot of text and not so much code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4d2a7e-05d4-40e6-9358-b0830bfef923",
   "metadata": {},
   "source": [
    "## 1.3 CodeT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75192e08-3dc9-421d-aa1b-2305a6c739a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')\n",
    "model = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a769a6f4-0a2f-4a15-9cb5-6097534f3689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test yourscreenshot for you\"_test_youtube\" : function ( )] ; #YouTube app}page.onclick (Page: Page:) {0 ] }'_test_youtube' : \"\"\"Test a YouTubetest_youtube(. test_youtube }\n"
     ]
    }
   ],
   "source": [
    "# Define the input text with the updated prompt\n",
    "text = \"\"\"\n",
    "Generate a Python script using the Playwright library to perform UI testing on YouTube. \n",
    "The script should:\n",
    "1. Navigate to the YouTube webpage.\n",
    "2. Accept cookies.\n",
    "3. Search for a video.\n",
    "4. Take a screenshot of the search results.\n",
    "The function should start as follows:\n",
    "def test_youtube(page: Page):\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "input_ids = inputs.input_ids\n",
    "attention_mask = inputs.attention_mask\n",
    "\n",
    "# Generate the Python script\n",
    "generated_ids = model.generate(input_ids = input_ids,\n",
    "                               attention_mask = attention_mask,\n",
    "                               max_length=500,\n",
    "                               do_sample=True)  # Adjust max_length as needed\n",
    "generated_code = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated Python script\n",
    "print(generated_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6a5f41-435e-4899-9164-64ba243ba8f5",
   "metadata": {},
   "source": [
    "Like in gpt. To model do something, but the model it is not fitted do this, it can do better. Sometimes to model predict random stuff."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27fc8cd-317e-4ba4-ba11-3b47d3aac8d5",
   "metadata": {},
   "source": [
    "## 1.4 Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284d1750-4cc0-47d3-b0a3-0f35ec32b513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef339deca9fb4359bbc2e391d6787d2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db8ebb5d7b1941b880ab42f86a3e5d31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:  25%|##4       | 1.13G/4.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "411bfa7996234c54ba72ec26cbd9f303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.3\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5ee911-ec57-4b21-8504-8a957933581d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee4fe28-5df1-4d7c-a67a-507472ca1415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input text with the updated prompt\n",
    "text = \"\"\"\n",
    "Generate a Python script using the Playwright library to perform UI testing on YouTube. \n",
    "The script should:\n",
    "1. Navigate to the YouTube webpage.\n",
    "2. Accept cookies.\n",
    "3. Search for a video.\n",
    "4. Take a screenshot of the search results.\n",
    "The function should start as follows:\n",
    "def test_youtube(page: Page):\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "# input_ids = inputs.input_ids\n",
    "# attention_mask = inputs.attention_mask\n",
    "\n",
    "# # Generate the Python script\n",
    "# generated_ids = model.generate(input_ids = input_ids,\n",
    "#                                attention_mask = attention_mask,\n",
    "#                                max_length=500,\n",
    "#                                do_sample=True)  # Adjust max_length as needed\n",
    "# generated_code = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# # Print the generated Python script\n",
    "# print(generated_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0a68a1-7d1b-4741-9b89-d6a3914f56b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b823c7f-c5a6-4b0a-ab31-d8ad49d79c78",
   "metadata": {},
   "source": [
    "# 2. Multimodal Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05c3d620-22c5-4a04-aafa-7b0a011c30d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.13.3)\n",
      "Collecting huggingface-hub<1.0,>=0.23.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.5.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n",
      "Downloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.6/402.6 kB\u001b[0m \u001b[31m274.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.5.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (785 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m785.0/785.0 kB\u001b[0m \u001b[31m138.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m152.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m114.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.23.4 regex-2024.5.15 safetensors-0.4.3 tokenizers-0.19.1 transformers-4.41.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c8902d5-2b0b-4faf-a4b9-0fad1ad96703",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LLaVAModel' from 'transformers' (/opt/conda/lib/python3.11/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLaVAModel, LLaVATokenizer\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'LLaVAModel' from 'transformers' (/opt/conda/lib/python3.11/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup, Comment\n",
    "from PIL import Image\n",
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "import torch\n",
    "import requests\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbb4a082-d24f-4f32-a715-50b87681b9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e937eb0-338e-4ea9-a13f-11c47e12fc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_html_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "    return html_content\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def preprocess_html(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Remove head element\n",
    "    head_tag = soup.find('head')\n",
    "    if head_tag:\n",
    "        head_tag.decompose()\n",
    "\n",
    "    # Remove footer element\n",
    "    footer_tag = soup.find('footer')\n",
    "    if footer_tag:\n",
    "        footer_tag.decompose()\n",
    "\n",
    "    # Remove nav elements\n",
    "    # for nav_tag in soup.find_all('nav'):\n",
    "    #     nav_tag.decompose()\n",
    "\n",
    "    # Remove aside elements\n",
    "    # for aside_tag in soup.find_all('aside'):\n",
    "    #     aside_tag.decompose()\n",
    "\n",
    "    # Remove header elements\n",
    "    for header_tag in soup.find_all('header'):\n",
    "        header_tag.decompose()\n",
    "\n",
    "    # Remove comments\n",
    "    for comment in soup.find_all(text=lambda text: isinstance(text, Comment)):\n",
    "        comment.extract()\n",
    "\n",
    "    return soup.prettify()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52c3e2f6-d187-4e43-87a5-5e912980c779",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_336/4149852038.py:34: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  for comment in soup.find_all(text=lambda text: isinstance(text, Comment)):\n"
     ]
    }
   ],
   "source": [
    "# Load the content\n",
    "steps = '1) Im Ordner Zentralle Dienste öffne Personal-Controlling'\n",
    "expected_result = 'Personal-Controlling wird geöffnet'\n",
    "html_file_path = './inputs/test1/capture__93f17294-cfb4-438b-afb2-814005784206.html'\n",
    "html_content = load_html_file(html_file_path)\n",
    "html_reduce = preprocess_html(html_content)\n",
    "# Load an image\n",
    "image_path = './inputs/test1/capture__c09cb103-efb8-4001-88bf-254174cc6ed1.png'  # Replace with your image path\n",
    "image = Image.open(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30d176b2-2d8a-49ae-9241-d78d2e376a71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21416"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f842576-bb19-446a-974d-6629aefb70e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4308"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(html_reduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58892a02-ae81-4504-a062-78ac9ab32655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image format: PNG\n",
      "Image size: (1280, 720)\n",
      "Image mode: RGB\n"
     ]
    }
   ],
   "source": [
    "# Display information about the image\n",
    "print(f\"Image format: {image.format}\")\n",
    "print(f\"Image size: {image.size}\")\n",
    "print(f\"Image mode: {image.mode}\")\n",
    "\n",
    "# # Show the image\n",
    "# image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cfd2370-832c-478c-a2d9-8d788151093e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\n",
    "        f\"Steps:\\n{steps}\\n\\n\"\n",
    "        f\"Expected Result:\\n{expected_result}\\n\\n\"\n",
    "        f\"HTML content:\\n{html_content}\\n\\n\"\n",
    "        f\"Generate Python code based on the above inputs.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3ea05053-70f1-4de7-94b5-639ffd12d343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e766c321-b7e4-4a3c-8181-742d42412ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46ae0b94b1874e21925d05f901a8494d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hf_path = 'tinyllava/TinyLLaVA-Phi-2-SigLIP-3.1B'\n",
    "model = AutoModelForCausalLM.from_pretrained(hf_path, trust_remote_code=True)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f0b4a2-3ab6-4cc2-bd04-f2f3f32c0eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LLaVATokenizer.from_pretrained('llava-1.6v-mistral-7b-hf')\n",
    "model = LLaVAModel.from_pretrained('llava-1.6v-mistral-7b-hf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc951a9-88d9-4d55-854e-6f37bb495e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = model.config\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_path, use_fast=False, model_max_length = config.tokenizer_model_max_length,padding_side = config.tokenizer_padding_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66e0e092-e402-43ad-a82b-781fdbe4f8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"What are these?\"\n",
    "# image_url=\"./inputs/test1/capture__c09cb103-efb8-4001-88bf-254174cc6ed1.png\"\n",
    "output_text, genertaion_time = model.chat(prompt=prompt, image=image, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f91a48-3d9d-4812-90c2-f025891f0d14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
