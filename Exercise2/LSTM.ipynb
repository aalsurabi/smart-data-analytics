{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad642c71-642c-468d-9ac5-640d9e0b97a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: ray in /opt/conda/lib/python3.11/site-packages (2.23.0)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.11/site-packages (from ray) (8.1.7)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from ray) (3.13.3)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.11/site-packages (from ray) (4.21.1)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from ray) (1.0.7)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from ray) (24.0)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /opt/conda/lib/python3.11/site-packages (from ray) (4.25.3)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from ray) (6.0.1)\n",
      "Requirement already satisfied: aiosignal in /opt/conda/lib/python3.11/site-packages (from ray) (1.3.1)\n",
      "Requirement already satisfied: frozenlist in /opt/conda/lib/python3.11/site-packages (from ray) (1.4.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from ray) (2.31.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.11/site-packages (from jsonschema->ray) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.11/site-packages (from jsonschema->ray) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.11/site-packages (from jsonschema->ray) (0.34.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from jsonschema->ray) (0.18.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->ray) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->ray) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->ray) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->ray) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install ray\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import ray\n",
    "from ray import train, tune\n",
    "from ray.train import Checkpoint\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import os\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414f2260-0164-4a26-b02f-73f80da5167e",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d6f1b98-2219-4767-a0a5-595839156464",
   "metadata": {},
   "outputs": [],
   "source": [
    "FD001_train = pd.read_csv('data/train_FD001.txt', sep='\\s+', header=None)\n",
    "FD001_test = pd.read_csv('data/test_FD001.txt', sep='\\s+', header=None)\n",
    "FD002_train = pd.read_csv('data/train_FD002.txt', sep='\\s+', header=None)\n",
    "FD002_test = pd.read_csv('data/test_FD002.txt', sep='\\s+', header=None)\n",
    "FD003_train = pd.read_csv('data/train_FD003.txt', sep='\\s+', header=None)\n",
    "FD003_test = pd.read_csv('data/test_FD003.txt', sep='\\s+', header=None)\n",
    "FD004_train = pd.read_csv('data/train_FD004.txt', sep='\\s+', header=None)\n",
    "FD004_test = pd.read_csv('data/test_FD004.txt', sep='\\s+', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "902d0635-b365-4807-a2fd-0b9720f1d6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "FD001_test_targets = pd.read_csv('data/RUL_FD001.txt',sep='\\s+', header=None, names=[\"RUL\"])\n",
    "FD002_test_targets = pd.read_csv('data/RUL_FD002.txt',sep='\\s+', header=None, names=[\"RUL\"])\n",
    "FD003_test_targets = pd.read_csv('data/RUL_FD003.txt',sep='\\s+', header=None, names=[\"RUL\"])\n",
    "FD004_test_targets = pd.read_csv('data/RUL_FD004.txt',sep='\\s+', header=None, names=[\"RUL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae35e539-1ef6-43f7-a8da-9a467e827d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column names\n",
    "index_columns_names =  [\"unit_number\",\"cycle\"]\n",
    "operational_settings_columns_names = [\"operational_setting_\"+str(i) for i in range(1,4)]\n",
    "sensor_measure_columns_names =[\"sensor_\"+str(i) for i in range(1,22)]\n",
    "input_file_column_names = index_columns_names + operational_settings_columns_names + sensor_measure_columns_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "694e6bf1-1031-4d83-b1a0-5d4646c5fe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "FD001_train.columns = input_file_column_names\n",
    "FD001_test.columns = input_file_column_names\n",
    "FD002_train.columns = input_file_column_names\n",
    "FD002_test.columns = input_file_column_names\n",
    "FD003_train.columns = input_file_column_names\n",
    "FD003_test.columns = input_file_column_names\n",
    "FD004_train.columns = input_file_column_names\n",
    "FD004_test.columns = input_file_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a31192db-6db4-486f-9359-1a216d873e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "\n",
    "# FD001 data\n",
    "datasets['FD001'] = {\n",
    "    'train': FD001_train,\n",
    "    'test': FD001_test,\n",
    "    'test_targets': FD001_test_targets\n",
    "}\n",
    "\n",
    "# FD002 data\n",
    "datasets['FD002'] = {\n",
    "    'train': FD002_train,\n",
    "    'test': FD002_test,\n",
    "    'test_targets': FD002_test_targets\n",
    "}\n",
    "\n",
    "# FD003 data\n",
    "datasets['FD003'] = {\n",
    "    'train': FD003_train,\n",
    "    'test': FD003_test,\n",
    "    'test_targets': FD003_test_targets\n",
    "}\n",
    "\n",
    "# FD004 data\n",
    "datasets['FD004'] = {\n",
    "    'train': FD004_train,\n",
    "    'test': FD004_test,\n",
    "    'test_targets': FD004_test_targets\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2aa8a45c-2ad3-400f-b587-bbcf8b081a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FD001 Train dataset size: 20631\n",
      "FD001 Test_target dataset size: 100\n",
      "FD001 test data: Minimum of maximum cycle values: 31\n",
      "FD002 Train dataset size: 53759\n",
      "FD002 Test_target dataset size: 259\n",
      "FD002 test data: Minimum of maximum cycle values: 21\n",
      "FD003 Train dataset size: 24720\n",
      "FD003 Test_target dataset size: 100\n",
      "FD003 test data: Minimum of maximum cycle values: 38\n",
      "FD004 Train dataset size: 61249\n",
      "FD004 Test_target dataset size: 248\n",
      "FD004 test data: Minimum of maximum cycle values: 19\n"
     ]
    }
   ],
   "source": [
    "for engine_id, engine_data in datasets.items():\n",
    "    print(f\"{engine_id} Train dataset size: {len(engine_data['train'])}\")\n",
    "    print(f\"{engine_id} Test_target dataset size: {len(engine_data['test_targets'])}\")\n",
    "    engine_df = engine_data[\"test\"]\n",
    "    grouped_by_unit = engine_df.groupby(by=\"unit_number\")\n",
    "    max_cycle = grouped_by_unit[\"cycle\"].max()\n",
    "    min_max_cycle = max_cycle.min()\n",
    "    engine_data[\"max_window_size\"] = min_max_cycle\n",
    "    print(f\"{engine_id} test data: Minimum of maximum cycle values: {min_max_cycle}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433e4614-4682-4f03-9a4b-e614dedfb29f",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855a56a2-4ec0-4a1a-91ad-d3cac61b25b0",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6713357-1366-4310-9008-c860a20f34c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calculate_rul(df, initial_rul=0):\n",
    "    \"\"\"\n",
    "    Calculates target RUL for a dataframe. If initial_rul is != 0 piece-wise linear degradation is calculated \n",
    "    (Initially, RUL is set to constant value until degradation starts). Otherwise, RUL is linear degradation \n",
    "    and starts with max_cycle number for a motor unit.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame containing the data\n",
    "    - initial_rul: Initial constant RUL value before degradation starts\n",
    "    \n",
    "    Returns:\n",
    "    - numpy array containing the RUL values for the entire dataframe\n",
    "    \"\"\"\n",
    "    grouped = df.groupby(\"unit_number\")\n",
    "    ruls = []\n",
    "\n",
    "    for _, unit in grouped:\n",
    "        max_cycle = unit.shape[0]\n",
    "        targets = np.arange(max_cycle, -1, -1)[:-1]  # create array from max_cycle-1 to 0\n",
    "        if initial_rul > 0:\n",
    "            targets = np.clip(targets, None, initial_rul)\n",
    "        ruls.append(targets)\n",
    "    \n",
    "    return np.concatenate(ruls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf080dd7-798a-4229-8d6d-42e30a34d62d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scale(df, scaler, train=True):\n",
    "    df_scaled = df.copy()\n",
    "    columns_to_scale = operational_settings_columns_names + sensor_measure_columns_names\n",
    "    if train: \n",
    "        df_scaled[columns_to_scale] = scaler.fit_transform(df_scaled[columns_to_scale])\n",
    "    else:\n",
    "        df_scaled[columns_to_scale] = scaler.transform(df_scaled[columns_to_scale])\n",
    "    return df_scaled, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4abf888-a684-4a0f-90f0-191e7b281f80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_sequence(df, window_size=30, stride=1):\n",
    "    grouped = df.groupby(\"unit_number\")\n",
    "    X_processed = []\n",
    "    y_processed = []\n",
    "    for _, unit in grouped:\n",
    "        #unit_data = unit.sort_values(by=\"cycle\", ascending=True)\n",
    "        target = unit[\"rul\"]\n",
    "        windows = extract_windows(unit.drop([\"cycle\", \"rul\"],axis=1), window_size, stride)\n",
    "        X_processed.append(windows)\n",
    "        targets_for_windows = target[-windows.shape[0]::] #the target for a window is target rul of value at the end of window. So the first num_windows values of target care cut off\n",
    "        y_processed.append(targets_for_windows)\n",
    "    X_processed = np.concatenate(X_processed) # shape (number of total extracted windows,window size,  features)\n",
    "    y_processed = np.concatenate(y_processed) # shape (number of total extracted windows)\n",
    "    return X_processed, y_processed\n",
    "    \n",
    "def extract_windows(data, window_size, stride):\n",
    "    if data.shape[0] < window_size:\n",
    "            raise AssertionError(\"Window length is larger than sequence length \")\n",
    "    windows = sliding_window_view(data, window_shape=(window_size, data.shape[1])).squeeze() #squeeze to remove dimension with 1\n",
    "    if stride != 1:\n",
    "        windows = windows[::stride]\n",
    "    return windows  \n",
    "\n",
    "\n",
    "# Take last window for final prediction\n",
    "def generate_test_sequence(df,  window_size=30, stride=1):\n",
    "    grouped = df.groupby(\"unit_number\")\n",
    "    X_processed = []\n",
    "    for _, unit in grouped:\n",
    "        #unit_data = unit.sort_values(by=\"cycle\", ascending=True)\n",
    "        #windows = extract_windows(unit.drop([\"cycle\"],axis=1), window_size, stride)\n",
    "        #windows = windows[-n_windows:]  # take only last n windows\n",
    "        window = unit.drop([\"cycle\"], axis=1)[-window_size:]\n",
    "        X_processed.append(window)\n",
    "       \n",
    "    X_processed = np.stack(X_processed) # shape (number_units, window_size, features)\n",
    "\n",
    "    return X_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da889b9b-cb50-4dfe-a2e4-350b9464efc3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Dataset classes for Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a9371ab-81de-434f-95ff-684158cd97d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CMAPPSDataset(Dataset):\n",
    "    def __init__(self, data, scaler, columns_to_drop, window_size, stride, initial_rul = 0, train=True):\n",
    "        self.data = data\n",
    "        if scaler is None:\n",
    "            self.scaler = StandardScaler()\n",
    "        elif isinstance(scaler, type):\n",
    "            self.scaler = scaler()  # scaler is a class, so we instantiate it\n",
    "        else:\n",
    "            self.scaler = scaler  # scaler is already an instance\n",
    "        \n",
    "\n",
    "\n",
    "        # if train is true scaler does fit_transform(). Else only transform()\n",
    "\n",
    "        self.X, self.scaler  = scale(self.data, scaler=self.scaler, train=train)\n",
    "        self.X = self.X.drop(columns_to_drop,axis=1)\n",
    "        self.X[\"rul\"] = calculate_rul(self.data, initial_rul = initial_rul)\n",
    "        self.X_seq, self.y_seq = generate_sequence(self.X, window_size=window_size, stride=stride)\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_seq)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "         sample =  torch.tensor(self.X_seq[idx], dtype=torch.float)\n",
    "         target =  torch.tensor(self.y_seq[idx], dtype=torch.float)\n",
    "         return sample, target\n",
    "\n",
    "    def get_scaler(self):\n",
    "        return self.scaler\n",
    "\n",
    "\n",
    "class CMAPPSTestDataset(Dataset):\n",
    "    def __init__(self, data, targets, scaler, columns_to_drop, window_size, stride, n_windows=1):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.scaler = scaler #is already instantiated scaler\n",
    "        self.y = targets.squeeze()\n",
    "\n",
    "        # if train is true scaler is fit and transform. Else only transform\n",
    "        self.X, self.scaler  = scale(self.data, scaler=self.scaler, train=train)\n",
    "        self.X = self.X.drop(columns_to_drop,axis=1)\n",
    "        self.X_seq = generate_test_sequence(self.X, window_size=window_size, stride=stride)\n",
    "        \n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_seq)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "         sample =  torch.tensor(self.X_seq[idx], dtype=torch.float)\n",
    "         target =  torch.tensor(self.y[idx], dtype=torch.float)\n",
    "         return sample, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df209baf-6197-42c5-b002-92e67b73b4d6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Preprocessing tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6dcebca-e49d-4e66-9955-5e6484d4c807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rul piecewise-linear:  [130 130 130 130 130 130 130 130 130 130 130 130 130 130 130 130 130 130\n",
      " 130 130 130 130 130 130 130 130 130 130 130 130 130 130 130 130 130 130\n",
      " 130 130 130 130 130 130 130 130 130 130 130 130 130 130 130 130 130 130\n",
      " 130 130 130 130 130 130 130 130 130 129 128 127 126 125 124 123 122 121\n",
      " 120 119 118 117 116 115 114 113 112 111 110 109 108 107 106 105 104 103\n",
      " 102 101 100  99  98  97  96  95  94  93  92  91  90  89  88  87  86  85\n",
      "  84  83  82  81  80  79  78  77  76  75  74  73  72  71  70  69  68  67\n",
      "  66  65  64  63  62  61  60  59  58  57  56  55  54  53  52  51  50  49\n",
      "  48  47  46  45  44  43  42  41  40  39  38  37  36  35  34  33  32  31\n",
      "  30  29  28  27  26  25  24  23  22  21  20  19  18  17  16  15  14  13\n",
      "  12  11  10   9   8   7   6   5   4   3   2   1]\n",
      "rul linear:  [192 191 190 189 188 187 186 185 184 183 182 181 180 179 178 177 176 175\n",
      " 174 173 172 171 170 169 168 167 166 165 164 163 162 161 160 159 158 157\n",
      " 156 155 154 153 152 151 150 149 148 147 146 145 144 143 142 141 140 139\n",
      " 138 137 136 135 134 133 132 131 130 129 128 127 126 125 124 123 122 121\n",
      " 120 119 118 117 116 115 114 113 112 111 110 109 108 107 106 105 104 103\n",
      " 102 101 100  99  98  97  96  95  94  93  92  91  90  89  88  87  86  85\n",
      "  84  83  82  81  80  79  78  77  76  75  74  73  72  71  70  69  68  67\n",
      "  66  65  64  63  62  61  60  59  58  57  56  55  54  53  52  51  50  49\n",
      "  48  47  46  45  44  43  42  41  40  39  38  37  36  35  34  33  32  31\n",
      "  30  29  28  27  26  25  24  23  22  21  20  19  18  17  16  15  14  13\n",
      "  12  11  10   9   8   7   6   5   4   3   2   1]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "unit_1_df = FD001_train[FD001_train[\"unit_number\"]==1].copy()\n",
    "print(\"rul piecewise-linear: \" , calculate_rul(unit_1_df, initial_rul=130))\n",
    "print(\"rul linear: \",  calculate_rul(unit_1_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7b3bb7a-2eb5-401d-a2fa-490f642a5a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2]\n",
      " [ 3  4]\n",
      " [ 5  6]\n",
      " [ 7  8]\n",
      " [ 9 10]\n",
      " [11 12]\n",
      " [13 14]\n",
      " [15 16]\n",
      " [17 18]\n",
      " [19 20]]\n",
      "(10, 2)\n",
      "\n",
      " Extracted windows  with window size 2\n",
      " [[[ 1  2]\n",
      "  [ 3  4]]\n",
      "\n",
      " [[ 3  4]\n",
      "  [ 5  6]]\n",
      "\n",
      " [[ 5  6]\n",
      "  [ 7  8]]\n",
      "\n",
      " [[ 7  8]\n",
      "  [ 9 10]]\n",
      "\n",
      " [[ 9 10]\n",
      "  [11 12]]\n",
      "\n",
      " [[11 12]\n",
      "  [13 14]]\n",
      "\n",
      " [[13 14]\n",
      "  [15 16]]\n",
      "\n",
      " [[15 16]\n",
      "  [17 18]]\n",
      "\n",
      " [[17 18]\n",
      "  [19 20]]]\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(1,21).reshape(10,2) #first dimension is time step\n",
    "print(a)\n",
    "print(a.shape)\n",
    "b = np.arange(1,11)\n",
    "X = extract_windows(a, window_size=2, stride=1)\n",
    "print(\"\\n Extracted windows  with window size 2\\n\", X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccd13dc7-ef68-4e2a-ae06-e175b98f0515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20631, 27)\n",
      "(20631,)\n",
      "(20631, 5)\n",
      "   unit_number  cycle  sensor_2  sensor_17  rul\n",
      "0            1      1 -1.721725  -0.781710  192\n",
      "1            1      2 -1.061780  -0.781710  191\n",
      "2            1      3 -0.661813  -2.073094  190\n",
      "3            1      4 -0.661813  -0.781710  189\n",
      "4            1      5 -0.621816  -0.136018  188\n"
     ]
    }
   ],
   "source": [
    "data = FD001_train.copy()\n",
    "\n",
    "X= data\n",
    "X[\"rul\"] = calculate_rul(data)\n",
    "print(X.shape)\n",
    "print(X[\"rul\"].shape)\n",
    "\n",
    "X, _ = scale(X, StandardScaler())\n",
    "columns_to_drop_test = ['operational_setting_1', 'operational_setting_2', 'operational_setting_3', \n",
    "                   'sensor_1', 'sensor_3', 'sensor_4', 'sensor_5', 'sensor_6', 'sensor_7', 'sensor_8', 'sensor_9',\n",
    "                   'sensor_10', 'sensor_11', 'sensor_12', 'sensor_13', 'sensor_14', 'sensor_15', 'sensor_16', 'sensor_18', 'sensor_19', 'sensor_20', 'sensor_21']\n",
    "X = X.drop( columns_to_drop_test, axis=1)\n",
    "print(X.shape)\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a50dd1fc-b7de-4360-9263-e76ee1b35249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17731, 30, 3)\n",
      "(17731,)\n"
     ]
    }
   ],
   "source": [
    "X_seq, y_seq = generate_sequence(X, window_size=30, stride=1)\n",
    "print(X_seq.shape)\n",
    "print(y_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40d151c6-b1d4-4bd6-80e8-f5af0adaed69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13096, 26)\n",
      "Target shape:  (100, 1)\n",
      "Test seq shape (100, 30, 25)\n"
     ]
    }
   ],
   "source": [
    "data = FD001_test.copy()\n",
    "\n",
    "X_seq = generate_test_sequence(data, window_size=30, stride=1)\n",
    "print(data.shape)\n",
    "print(\"Target shape: \", FD001_test_targets.shape)\n",
    "print(\"Test seq shape\", X_seq.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e9d649c-196c-42d5-97e2-3da0446cd07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17731, 30, 25)\n",
      "(17731,)\n",
      "(100, 30, 25)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "dataset = CMAPPSDataset(FD001_train, StandardScaler, [], window_size=30, stride=1, initial_rul = 130, train=True)\n",
    "scaler = dataset.get_scaler()\n",
    "print(dataset.X_seq.shape)\n",
    "print(dataset.y_seq.shape)\n",
    "test_dataset = CMAPPSTestDataset(FD001_test, FD001_test_targets, scaler, [], window_size=30, stride=1)\n",
    "print(test_dataset.X_seq.shape)\n",
    "print(test_dataset.y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e693c9c7-a947-445f-ab7b-d929492f6e36",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56448cbb-7c42-4c19-a0d2-70d43f482828",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, lstm_hidden_sizes=[64], fc_hidden_sizes=[64], output_size=1, activation=nn.ReLU, dropout=0.0):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.lstm_hidden_sizes = lstm_hidden_sizes\n",
    "        self.fc_hidden_sizes = fc_hidden_sizes\n",
    "        self.lstm_layers = []\n",
    "        self.fc_layers = []\n",
    "        self.activation = activation()\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # LSTM layers\n",
    "        num_lstm_layers = len(lstm_hidden_sizes)\n",
    "        lstm_input_size = input_size\n",
    "        for i in range(num_lstm_layers):\n",
    "            hidden_size = lstm_hidden_sizes[i] if i < len(lstm_hidden_sizes) else lstm_hidden_sizes[-1]\n",
    "            self.lstm_layers.append(nn.LSTM(lstm_input_size, hidden_size, batch_first=True, dropout=dropout))\n",
    "            lstm_input_size = hidden_size\n",
    "        \n",
    "        self.lstm = nn.ModuleList(self.lstm_layers)\n",
    "        \n",
    "        # FC layers\n",
    "        num_fc_layers = len(fc_hidden_sizes)\n",
    "        fc_input_size = lstm_hidden_sizes[-1] if lstm_hidden_sizes else input_size\n",
    "        for i in range(num_fc_layers):\n",
    "            hidden_size = fc_hidden_sizes[i] if i < len(fc_hidden_sizes) else fc_hidden_sizes[-1]\n",
    "            self.fc_layers.append(nn.Linear(fc_input_size, hidden_size))\n",
    "            self.fc_layers.append(nn.Dropout(dropout))\n",
    "            fc_input_size = hidden_size\n",
    "        \n",
    "        self.fc = nn.ModuleList(self.fc_layers)\n",
    "        self.output_layer = nn.Linear(fc_input_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # LSTM layers\n",
    "        for i, lstm_layer in enumerate(self.lstm):\n",
    "            x, _ = lstm_layer(x)\n",
    "        \n",
    "        # FC layers\n",
    "        for i, fc_layer in enumerate(self.fc):\n",
    "            x = fc_layer(x)\n",
    "            x = self.activation(x) if i < len(self.fc_layers) - 1 else x\n",
    "        \n",
    "        out = self.output_layer(x[:, -1, :])  # Taking the last output from the sequence\n",
    "        return out.squeeze()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39a0c76-2fd8-4618-99cc-d6ef61c09f4d",
   "metadata": {},
   "source": [
    "## Hybrid Deep Neural Network\n",
    "https://ieeexplore.ieee.org/document/8683763\n",
    "<img src=\"image/HDNN.png\" alt=\"Alt text\" title=\"Optional title\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e64699f1-69b8-4026-9b6e-8b7b8f949f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input is (batch, window_size, features)\n",
    "\n",
    "class HybridDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, window_size, lstm_hidden_sizes=[32, 32, 64],  output_size=1, activation=nn.ReLU, dropout=0.0,  kernel_sizes = [(10,1), (10,1), (3,1)], out_channels = [10, 10, 1], kernel_max_pool_size=(2,1), fc_hidden_sizes = [100, 100]):\n",
    "        super(HybridDeepNeuralNetwork, self).__init__()\n",
    "        self.feature_dim = input_size\n",
    "        num_lstm_layers = len(lstm_hidden_sizes)\n",
    "        lstm_input_size =  self.feature_dim\n",
    "        self.lstm_layers = []\n",
    "        for i in range(num_lstm_layers):\n",
    "            hidden_size = lstm_hidden_sizes[i] if i < len(lstm_hidden_sizes) else lstm_hidden_sizes[-1]\n",
    "            self.lstm_layers.append(nn.LSTM(lstm_input_size, hidden_size, batch_first=True, dropout=dropout))\n",
    "            lstm_input_size = hidden_size\n",
    "        \n",
    "        self.lstm = nn.ModuleList(self.lstm_layers)\n",
    "\n",
    "\n",
    "        self.conv_layers = []\n",
    "        num_conv_layers = len(kernel_sizes)\n",
    "        in_channels = [1] + out_channels[:-1]\n",
    "        for i in range(num_conv_layers):\n",
    "            self.conv_layers.append(nn.Conv2d(in_channels=in_channels[i], out_channels=out_channels[i], kernel_size=kernel_sizes[i], padding='same'))\n",
    "            if i != (num_conv_layers-1):\n",
    "                self.conv_layers.append(nn.MaxPool2d(kernel_size=kernel_max_pool_size))\n",
    "        self.conv = nn.ModuleList(self.conv_layers)\n",
    "\n",
    "         # FC layers\n",
    "        num_fc_layers = len(fc_hidden_sizes)\n",
    "        fc_input_size = lstm_hidden_sizes[-1] + ( window_size // (2 ** (num_conv_layers - 1))  * self.feature_dim)\n",
    "        self.fc_layers = []\n",
    "        for i in range(num_fc_layers):\n",
    "            hidden_size = fc_hidden_sizes[i] if i < len(fc_hidden_sizes) else fc_hidden_sizes[-1]\n",
    "            self.fc_layers.append(nn.Linear(fc_input_size, hidden_size))\n",
    "            self.fc_layers.append(nn.Tanh())\n",
    "            fc_input_size = hidden_size\n",
    "        \n",
    "        self.fc = nn.ModuleList(self.fc_layers)\n",
    "        self.output_layer = nn.Linear(fc_input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "         # LSTM forward pass\n",
    "        # x has shape (batch, window_size, feature_dim)\n",
    "        lstm_out = x\n",
    "        for lstm_layer in self.lstm_layers:\n",
    "            lstm_out, _ = lstm_layer(lstm_out)\n",
    "        \n",
    "        # Convolutional forward pass\n",
    "        conv_out = x.unsqueeze(1)  # Add a channel dimension\n",
    "        #conv_out = x.permute(0,2,1) # conv2d input shape (batch, channel, h x w)\n",
    "        for conv_layer in self.conv_layers:\n",
    "            conv_out = conv_layer(conv_out)\n",
    "           # print(\"conv out\", conv_out.shape)\n",
    "            \n",
    "        conv_out_flat =  conv_out.reshape(conv_out.shape[0], -1)\n",
    "        combined= torch.cat([lstm_out[:, -1, :], conv_out_flat], dim=1)\n",
    "        # Fully connected layers forward pass\n",
    "        fc_out = combined\n",
    "        for fc_layer in self.fc_layers:\n",
    "            fc_out = fc_layer(fc_out)\n",
    "        out = self.output_layer(fc_out)\n",
    "        return out\n",
    "\n",
    "\n",
    "#tensor = torch.rand([11,30,15])\n",
    "#hdnn = HybridDeepNeuralNetwork(input_size=15, window_size=30)\n",
    "#hdnn(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc29bb6b-4222-4435-9e76-6530e608f9ca",
   "metadata": {},
   "source": [
    "# Train and evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c948949f-e91d-4e79-bcbc-db2a78c86914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def compute_s_score(rul_true, rul_pred):\n",
    "    diff = rul_pred - rul_true\n",
    "    return torch.mean(torch.where(diff < 0, torch.exp(-diff/13)-1, torch.exp(diff/10)-1))\n",
    "\n",
    "def reset_weights(m):\n",
    "  '''\n",
    "    Try resetting model weights to avoid\n",
    "    weight leakage.\n",
    "  '''\n",
    "  for layer in m.children():\n",
    "   if hasattr(layer, 'reset_parameters'):\n",
    "    #print(f'Reset trainable parameters of layer = {layer}')\n",
    "    layer.reset_parameters()\n",
    "       \n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs).squeeze()\n",
    "        if outputs.dim() == 0:  # Handle the case where outputs is a scalar\n",
    "            outputs = outputs.unsqueeze(0)\n",
    "        loss = criterion(outputs, targets)\n",
    "        running_loss +=loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    total_train_loss = running_loss / len(train_loader)\n",
    "    return total_train_loss\n",
    "\n",
    "\n",
    "def evaluate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    score = 0.0\n",
    "    #total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs).squeeze()\n",
    "            if outputs.dim() == 0:  # Handle the case where outputs is a scalar\n",
    "                outputs = outputs.unsqueeze(0)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss +=loss.item()  * inputs.size(0)\n",
    "            score += compute_s_score(outputs, targets).item()  * inputs.size(0)\n",
    "            #score = compute_s_score(outputs, targets)\n",
    "            #scores.append(score.item()) \n",
    "\n",
    "    total_loss = running_loss / len(val_loader.dataset)\n",
    "    mean_score = score / len(val_loader.dataset)\n",
    "    return total_loss, mean_score # Divide by the total number of samples\n",
    "\n",
    "def train_model(params: dict, data):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.manual_seed(42)\n",
    "    #train_val_split = GroupShuffleSplit(n_splits=2, train_size=.8, random_state=42)\n",
    "    #train_inds, val_inds = train_val_split.split(X,y, data[\"unit_number\"])\n",
    "    #print(len(train_inds))\n",
    "\n",
    "    #X_train, y_train = X.iloc[train_inds], y.iloc[train_inds]\n",
    "    #X_val, y_val = X.iloc[val_inds], y.iloc[val_inds]\n",
    "\n",
    "\n",
    "    # Cross Validation\n",
    "    n_splits=5\n",
    "    losses = []\n",
    "    scores = []\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    \n",
    "    for fold, (train_ids, val_ids) in enumerate(GroupKFold(n_splits=n_splits).split(data, y=None, groups= data[\"unit_number\"].copy())):\n",
    "\n",
    "        # Load datasets and perform preproccesing\n",
    "        #start_time = time.time()\n",
    "        train_subset = CMAPPSDataset( data=data.iloc[train_ids], scaler=params[\"scaler_cls\"], columns_to_drop=params[\"columns_to_drop\"], window_size=params[\"window_size\"], stride=params[\"stride\"], initial_rul = params[\"initial_rul\"], train=True)\n",
    "        train_scaler = train_subset.get_scaler()\n",
    "        \n",
    "        val_subset = CMAPPSDataset( data=data.iloc[val_ids], scaler=train_scaler, columns_to_drop=params[\"columns_to_drop\"], window_size=params[\"window_size\"], stride=params[\"stride\"],initial_rul =  params[\"initial_rul\"], train=False)\n",
    "        #end_time = time.time()\n",
    "        #elapsed_time = end_time - start_time\n",
    "        #print(\"Elapsed time:\", elapsed_time, \"seconds\")\n",
    "\n",
    "        train_loader = DataLoader(train_subset, batch_size=params[\"batch_size\"], shuffle=True, num_workers=8)\n",
    "        val_loader = DataLoader(val_subset, batch_size=params[\"batch_size\"], shuffle=True, num_workers=8)\n",
    "        \n",
    "\n",
    "        # Initialize model\n",
    "        input_sample, _ = train_subset[0]\n",
    "        input_size = input_sample.shape[-1]\n",
    "        model_params = params[\"model_params\"]\n",
    "        model_params[\"input_size\"] = input_size\n",
    "        # Initialize the model using the dictionary\n",
    "        model = params[\"model_cls\"](**params[\"model_params\"]).to(device)\n",
    "        model.apply(reset_weights)\n",
    "\n",
    "        #Initialize optimizer\n",
    "        optimizer = params[\"optimizer_cls\"](params = model.parameters(),lr=params[\"lr\"])\n",
    "\n",
    "\n",
    "        # Train loop\n",
    "        for epoch in range(params[\"num_epochs\"] ):\n",
    "            train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "            #print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss}\")\n",
    "\n",
    "        # Evaluate\n",
    "        val_loss, score = evaluate(model, val_loader, criterion, device)\n",
    "        #print(f\"Fold {fold + 1}/{n_splits}, Validation Loss  MSE: {val_loss}, RMSE: {np.sqrt(val_loss)}\")\n",
    "        losses.append(val_loss)\n",
    "        scores.append(score)\n",
    "\n",
    "    \n",
    "    avg_mse_loss = np.mean(losses)\n",
    "    rmse_loss = np.sqrt(avg_mse_loss)\n",
    "    avg_score =  np.mean(scores)\n",
    "    train.report({\"mse\": avg_mse_loss, 'rmse': rmse_loss, 'score': avg_score })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a665d12-a4ca-4eac-8a24-cd89c0dede90",
   "metadata": {},
   "source": [
    "## Final Training and Evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "467277d0-dc25-473f-83e1-6cacaceadda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_train_and_evaluate(params, train_data, test_data, targets ,save_dir=\"models\", name=\"final_model\"):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "   \n",
    "    # Parameters for training\n",
    "    train_dataset = CMAPPSDataset(data=train_data, scaler=params[\"scaler_cls\"], columns_to_drop=params[\"columns_to_drop\"], window_size=params[\"window_size\"], stride=params[\"stride\"], initial_rul=params[\"initial_rul\"], train=True)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True, num_workers=8)\n",
    "    scaler = train_dataset.get_scaler()\n",
    "\n",
    "    test_dataset = CMAPPSTestDataset(data=test_data, n_windows=1, targets=targets, scaler=scaler, columns_to_drop=params[\"columns_to_drop\"] , window_size=params[\"window_size\"],stride=params[\"stride\"])\n",
    "    test_loader = DataLoader(test_dataset, batch_size=params[\"batch_size\"], shuffle=False, num_workers=8)\n",
    "    test_loader_2 = DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=8)\n",
    "    \n",
    "    # Instantiate model\n",
    "    input_sample, _ = train_dataset[0]\n",
    "    input_size = input_sample.shape[-1]\n",
    "    model_params = params[\"model_params\"]\n",
    "    model_params[\"input_size\"] = input_size\n",
    "    # Initialize the model using the dictionary\n",
    "    model = params[\"model_cls\"](**params[\"model_params\"]).to(device)\n",
    "    model.apply(reset_weights)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = params[\"optimizer_cls\"](model.parameters(), lr=params[\"lr\"])   \n",
    "    # Train loop\n",
    "    print_frequency = max(params[\"num_epochs\"] // 10, 1) # show maximum of 10 lines\n",
    "    for epoch in range(params[\"num_epochs\"]):\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        if (epoch + 1) %  print_frequency == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{params['num_epochs']}, Train Loss: {train_loss}\")\n",
    "    \n",
    "     # Evaluate\n",
    "    test_mse_loss, test_score = evaluate(model, test_loader, criterion, device)\n",
    "    test_rmse_loss = np.sqrt(test_mse_loss)\n",
    "    print(f'Test Loss MSE: {test_mse_loss}, RMSE: {test_rmse_loss}, Score: {test_score}')\n",
    "    return model, test_rmse_loss, test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225f847f-4c11-4443-b3aa-91db21bf0d87",
   "metadata": {},
   "source": [
    "## Saving Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "444a22fb-0024-4ea9-b713-70adfdc55c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_params(params):\n",
    "    params_serializable = {}\n",
    "    for key, value in params.items():\n",
    "        if isinstance(value, dict):  # Check if value is a dictionary\n",
    "            params_serializable[key] = serialize_params(value)  # Recursively serialize nested dictionary\n",
    "        elif hasattr(value, \"__name__\"):  # Check if value has a __name__ attribute\n",
    "            params_serializable[key] = value.__name__  # Use the name of the class\n",
    "        else:\n",
    "            params_serializable[key] = value\n",
    "    return params_serializable\n",
    "\n",
    "def save_model_and_results(model, save_dir, name, params, rmse, score): # Save final model and results as json\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    final_model_path = os.path.join(save_dir, name + '.pth')\n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "    print(f\"Final model saved at {final_model_path}\")\n",
    "\n",
    "    results = {\n",
    "        \"params\": serialize_params(params),\n",
    "        \"rmse\": rmse,\n",
    "        \"score\": score\n",
    "    }\n",
    "\n",
    "    # Create file path\n",
    "    results_file_path = os.path.join(save_dir, name + \".json\")\n",
    "    \n",
    "    # Write results to JSON file\n",
    "    with open(results_file_path, \"w\") as f:\n",
    "        json.dump(results, f)\n",
    "    print(f\"Results saved at {results_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8857252-d355-48be-b54f-ba46b67272ad",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8020ea3-1e79-48aa-bb85-152ea69f6864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(engine_name, param_grid, filename):\n",
    "    train_data = datasets[\"FD001\"][\"train\"].copy()\n",
    "    test_data = datasets[\"FD001\"][\"test\"].copy()\n",
    "    test_targets = datasets[\"FD001\"][\"test_targets\"].copy()\n",
    "\n",
    "    scheduler = ASHAScheduler(\n",
    "        max_t=10,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "\n",
    "    tuner = tune.Tuner(\n",
    "        tune.with_resources(\n",
    "            tune.with_parameters(train_model, data=train_data), \n",
    "         resources={\"cpu\":8, \"gpu\": 1}),\n",
    "        tune_config=tune.TuneConfig(\n",
    "                metric=\"rmse\",\n",
    "                mode=\"min\",\n",
    "                scheduler=scheduler,\n",
    "                num_samples=1,\n",
    "            ),\n",
    "        param_space=param_grid)\n",
    "    results = tuner.fit()\n",
    "\n",
    "    best_result = results.get_best_result(\"rmse\")\n",
    "    print(\"Best trial RMSE validation loss (average over folds): {}\".format( best_result.metrics[\"rmse\"]))\n",
    "    best_params = best_result.config\n",
    "    print(\"Best trial config: {}\".format(best_params))\n",
    "    model, rmse, score = final_train_and_evaluate(best_params, train_data, test_data, test_targets , filename)\n",
    "    path = engine_name + \"_\" + filename\n",
    "    save_model_and_results(model=model, save_dir=\"results\", name=path, params=best_params, rmse=rmse, score=score)\n",
    "    return model, best_params, rmse, score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb22652d-80f0-46c5-bbc4-9ddf98c6ef10",
   "metadata": {},
   "source": [
    "# Baseline (FD001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed534bc8-deb0-4e5b-81ae-840bc43ab0b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-05-30 13:50:57</td></tr>\n",
       "<tr><td>Running for: </td><td>00:01:11.83        </td></tr>\n",
       "<tr><td>Memory:      </td><td>12.3/503.0 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=0<br>Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -44.477725800619446<br>Logical resource usage: 8.0/8 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A100)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    mse</th><th style=\"text-align: right;\">   rmse</th><th style=\"text-align: right;\">      score</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_799be_00000</td><td>TERMINATED</td><td>10.38.128.5:1992921</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         69.4857</td><td style=\"text-align: right;\">1978.27</td><td style=\"text-align: right;\">44.4777</td><td style=\"text-align: right;\">1.03618e+07</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-30 13:50:57,965\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n",
      "2024-05-30 13:50:57,968\tINFO tune.py:1007 -- Wrote the latest version of all result files and experiment state to '/home/jovyan/ray_results/train_model_2024-05-30_13-49-43' in 0.0082s.\n",
      "2024-05-30 13:50:57,972\tINFO tune.py:1039 -- Total run time: 71.85 seconds (71.82 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial RMSE validation loss (average over folds): 44.477725800619446\n",
      "Best trial config: {'model_cls': <class '__main__.LSTMModel'>, 'model_params': {'lstm_hidden_sizes': [32, 64], 'fc_hidden_sizes': [64], 'output_size': 1, 'activation': <class 'torch.nn.modules.activation.ReLU'>, 'dropout': 0.0, 'input_size': 25}, 'scaler_cls': <class 'sklearn.preprocessing._data.StandardScaler'>, 'window_size': 19, 'stride': 1, 'batch_size': 32, 'initial_rul': 0, 'optimizer_cls': <class 'torch.optim.adam.Adam'>, 'lr': 0.001, 'num_epochs': 10, 'columns_to_drop': []}\n",
      "Epoch 1/10, Train Loss: 3590.2291942654724\n",
      "Epoch 2/10, Train Loss: 1225.04967753956\n",
      "Epoch 3/10, Train Loss: 1097.8312189073029\n",
      "Epoch 4/10, Train Loss: 979.3373310497135\n",
      "Epoch 5/10, Train Loss: 910.0936358570041\n",
      "Epoch 6/10, Train Loss: 845.9922314111162\n",
      "Epoch 7/10, Train Loss: 781.7254119769422\n",
      "Epoch 8/10, Train Loss: 754.9501877219649\n",
      "Epoch 9/10, Train Loss: 686.842003037043\n",
      "Epoch 10/10, Train Loss: 631.3328025830824\n",
      "Test Loss MSE: 1171.55716796875, RMSE: 34.22801729532036, Score: 121.94973815917969\n",
      "Final model saved at results/FD001_baseline.pth\n",
      "Results saved at results/FD001_baseline.json\n",
      "LSTMModel(\n",
      "  (activation): ReLU()\n",
      "  (lstm): ModuleList(\n",
      "    (0): LSTM(25, 32, batch_first=True)\n",
      "    (1): LSTM(32, 64, batch_first=True)\n",
      "  )\n",
      "  (fc): ModuleList(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=2098353)\u001b[0m /opt/conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1040.)\n",
      "\u001b[36m(train_model pid=2098353)\u001b[0m   return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    }
   ],
   "source": [
    "columns_to_drop = []\n",
    "param_grid_baseline = {\n",
    "    \"model_cls\": LSTMModel,\n",
    "    \"model_params\": {\n",
    "            \"lstm_hidden_sizes\":  [32, 64],\n",
    "            \"fc_hidden_sizes\": [64],\n",
    "            \"output_size\": 1,\n",
    "            \"activation\": nn.ReLU,\n",
    "            \"dropout\": 0.0,\n",
    "        },\n",
    "    \"scaler_cls\": StandardScaler,\n",
    "    \"window_size\": 19,\n",
    "    \"stride\": 1,\n",
    "    \"batch_size\": 32,\n",
    "    \"initial_rul\": 0,\n",
    "    \"optimizer_cls\": optim.Adam,\n",
    "    \"lr\": 0.001,\n",
    "    \"num_epochs\": 10,\n",
    "    \"columns_to_drop\": columns_to_drop\n",
    "}\n",
    "\n",
    "\n",
    "results = grid_search(\"FD001\", param_grid_baseline, \"baseline\")\n",
    "print(results[0]) # print model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e9a3f5-03a9-44a8-862c-f777def83a88",
   "metadata": {},
   "source": [
    "## Removing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1aa5ae4d-6560-4c0b-9399-7ab06fa4322c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-05-30 13:52:24</td></tr>\n",
       "<tr><td>Running for: </td><td>00:01:09.36        </td></tr>\n",
       "<tr><td>Memory:      </td><td>12.7/503.0 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=0<br>Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -42.46546135769374<br>Logical resource usage: 8.0/8 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A100)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    mse</th><th style=\"text-align: right;\">   rmse</th><th style=\"text-align: right;\">      score</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_ae667_00000</td><td>TERMINATED</td><td>10.38.128.5:2028231</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         66.7161</td><td style=\"text-align: right;\">1803.32</td><td style=\"text-align: right;\">42.4655</td><td style=\"text-align: right;\">4.43942e+07</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-30 13:52:24,066\tINFO tune.py:1007 -- Wrote the latest version of all result files and experiment state to '/home/jovyan/ray_results/train_model_2024-05-30_13-51-14' in 0.0072s.\n",
      "2024-05-30 13:52:24,070\tINFO tune.py:1039 -- Total run time: 69.38 seconds (69.35 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial RMSE validation loss (average over folds): 42.46546135769374\n",
      "Best trial config: {'model_cls': <class '__main__.LSTMModel'>, 'model_params': {'lstm_hidden_sizes': [32, 64], 'fc_hidden_sizes': [64], 'output_size': 1, 'activation': <class 'torch.nn.modules.activation.ReLU'>, 'dropout': 0.0, 'input_size': 14}, 'scaler_cls': <class 'sklearn.preprocessing._data.StandardScaler'>, 'window_size': 19, 'stride': 1, 'batch_size': 32, 'initial_rul': 0, 'optimizer_cls': <class 'torch.optim.adam.Adam'>, 'lr': 0.001, 'num_epochs': 10, 'columns_to_drop': ['operational_setting_1', 'operational_setting_2', 'operational_setting_3', 'sensor_1', 'sensor_5', 'sensor_6', 'sensor_7', 'sensor_10', 'sensor_16', 'sensor_18', 'sensor_19']}\n",
      "Epoch 1/10, Train Loss: 3847.4211802458317\n",
      "Epoch 2/10, Train Loss: 1258.9469727287876\n",
      "Epoch 3/10, Train Loss: 1110.479840626741\n",
      "Epoch 4/10, Train Loss: 1014.2802479975495\n",
      "Epoch 5/10, Train Loss: 931.236155822443\n",
      "Epoch 6/10, Train Loss: 887.9522109752195\n",
      "Epoch 7/10, Train Loss: 815.7852019227421\n",
      "Epoch 8/10, Train Loss: 771.6495123508628\n",
      "Epoch 9/10, Train Loss: 766.8788347487134\n",
      "Epoch 10/10, Train Loss: 775.9820128798688\n",
      "Test Loss MSE: 1333.420126953125, RMSE: 36.51602561825595, Score: 165.7194625854492\n",
      "Final model saved at results/FD001_baseline_features_removed.pth\n",
      "Results saved at results/FD001_baseline_features_removed.json\n"
     ]
    }
   ],
   "source": [
    "columns_to_drop = ['operational_setting_1', 'operational_setting_2', 'operational_setting_3', \n",
    "               'sensor_1', 'sensor_5', 'sensor_6', 'sensor_7', \n",
    "               'sensor_10', 'sensor_16', 'sensor_18', 'sensor_19']\n",
    "param_grid_baseline_features_removed =  param_grid_baseline.copy()\n",
    "param_grid_baseline_features_removed[\"columns_to_drop\"] = columns_to_drop\n",
    "results = grid_search(\"FD001\", param_grid_baseline_features_removed, \"baseline_features_removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eb06d6-19b6-4cca-bd81-e42be24b7637",
   "metadata": {},
   "source": [
    "## piece-wise-RUL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "515d7824-e3fc-4dab-851c-19fafb63854a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-05-30 13:53:49</td></tr>\n",
       "<tr><td>Running for: </td><td>00:01:09.67        </td></tr>\n",
       "<tr><td>Memory:      </td><td>12.8/503.0 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=0<br>Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -20.093816715070353<br>Logical resource usage: 8.0/8 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A100)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    mse</th><th style=\"text-align: right;\">   rmse</th><th style=\"text-align: right;\">  score</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_e12e8_00000</td><td>TERMINATED</td><td>10.38.128.5:2063286</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         67.0428</td><td style=\"text-align: right;\">403.761</td><td style=\"text-align: right;\">20.0938</td><td style=\"text-align: right;\">13.0718</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-30 13:53:49,576\tINFO tune.py:1007 -- Wrote the latest version of all result files and experiment state to '/home/jovyan/ray_results/train_model_2024-05-30_13-52-39' in 0.0078s.\n",
      "2024-05-30 13:53:49,579\tINFO tune.py:1039 -- Total run time: 69.69 seconds (69.67 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial RMSE validation loss (average over folds): 20.093816715070353\n",
      "Best trial config: {'model_cls': <class '__main__.LSTMModel'>, 'model_params': {'lstm_hidden_sizes': [32, 64], 'fc_hidden_sizes': [64], 'output_size': 1, 'activation': <class 'torch.nn.modules.activation.ReLU'>, 'dropout': 0.0, 'input_size': 14}, 'scaler_cls': <class 'sklearn.preprocessing._data.StandardScaler'>, 'window_size': 19, 'stride': 1, 'batch_size': 32, 'initial_rul': 130, 'optimizer_cls': <class 'torch.optim.adam.Adam'>, 'lr': 0.001, 'num_epochs': 10, 'columns_to_drop': ['operational_setting_1', 'operational_setting_2', 'operational_setting_3', 'sensor_1', 'sensor_5', 'sensor_6', 'sensor_7', 'sensor_10', 'sensor_16', 'sensor_18', 'sensor_19']}\n",
      "Epoch 1/10, Train Loss: 1892.8032382518204\n",
      "Epoch 2/10, Train Loss: 324.4200558621936\n",
      "Epoch 3/10, Train Loss: 270.43335626728464\n",
      "Epoch 4/10, Train Loss: 243.8752752151878\n",
      "Epoch 5/10, Train Loss: 232.61589499766637\n",
      "Epoch 6/10, Train Loss: 220.17481066537025\n",
      "Epoch 7/10, Train Loss: 212.85409676725067\n",
      "Epoch 8/10, Train Loss: 203.08969234084435\n",
      "Epoch 9/10, Train Loss: 200.59532544406443\n",
      "Epoch 10/10, Train Loss: 194.4776930363757\n",
      "Test Loss MSE: 1097.882734375, RMSE: 33.134313549174365, Score: 174.3847155380249\n",
      "Final model saved at results/FD001_baseline_features_removed_and_rul_piecewise.pth\n",
      "Results saved at results/FD001_baseline_features_removed_and_rul_piecewise.json\n"
     ]
    }
   ],
   "source": [
    "param_grid_baseline_rul_piecewise =  param_grid_baseline_features_removed.copy()\n",
    "param_grid_baseline_rul_piecewise[\"initial_rul\"] = 130\n",
    "\n",
    "result = grid_search(\"FD001\", param_grid_baseline_rul_piecewise, \"baseline_features_removed_and_rul_piecewise\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609947f0-ab5c-46eb-88d5-12462c877ba9",
   "metadata": {},
   "source": [
    "## Trying to optimize more parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4098b91-b135-4d2c-a0c1-f11b44683c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['operational_setting_1', 'operational_setting_2', 'operational_setting_3', \n",
    "               'sensor_1', 'sensor_5', 'sensor_6', 'sensor_7', \n",
    "               'sensor_10', 'sensor_16', 'sensor_18', 'sensor_19']\n",
    "param_grid_lstm = {\n",
    "    \"model_cls\": LSTMModel,\n",
    "    \"model_params\": {\n",
    "            \"lstm_hidden_sizes\":  tune.grid_search([[32, 64], [32, 32, 64]] ),\n",
    "            \"fc_hidden_sizes\": tune.grid_search([32, 64], [32, 64, 128]]),\n",
    "            \"output_size\": 1,\n",
    "            \"activation\": nn.ReLU,\n",
    "            \"dropout\": tune.grid_search([0.3]),\n",
    "        },\n",
    "    \"scaler_cls\": StandardScaler,\n",
    "    \"window_size\": tune.grid_search([20, 30]),\n",
    "    \"stride\": 1,\n",
    "    \"batch_size\": tune.grid_search([64]),\n",
    "    \"initial_rul\": tune.grid_search([0, 125]),\n",
    "    \"optimizer_cls\": optim.Adam,\n",
    "    \"lr\": tune.grid_search([ 0.001]),\n",
    "    \"num_epochs\": tune.grid_search([50, 100]),\n",
    "    \"columns_to_drop\": tune.grid_search([ [], columns_to_drop])\n",
    "}\n",
    "results = grid_search(\"FD001\", param_grid_lstm, \"lstm\")\n",
    "print(results[0]) # print model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5669fd-cc3f-4fa6-a50a-ffa393f978d4",
   "metadata": {},
   "source": [
    "# HDNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "682a175a-7794-4a98-ad1b-a027341ffc46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-05-30 13:58:16</td></tr>\n",
       "<tr><td>Running for: </td><td>00:04:10.73        </td></tr>\n",
       "<tr><td>Memory:      </td><td>13.0/503.0 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=0<br>Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -41.30512308696933<br>Logical resource usage: 8.0/8 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A100)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    mse</th><th style=\"text-align: right;\">   rmse</th><th style=\"text-align: right;\">  score</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_145dd_00000</td><td>TERMINATED</td><td>10.38.128.5:2098353</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         248.076</td><td style=\"text-align: right;\">1706.11</td><td style=\"text-align: right;\">41.3051</td><td style=\"text-align: right;\">67.7216</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-30 13:58:16,504\tINFO tune.py:1007 -- Wrote the latest version of all result files and experiment state to '/home/jovyan/ray_results/train_model_2024-05-30_13-54-05' in 0.0100s.\n",
      "2024-05-30 13:58:16,508\tINFO tune.py:1039 -- Total run time: 250.74 seconds (250.72 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial RMSE validation loss (average over folds): 41.30512308696933\n",
      "Best trial config: {'model_cls': <class '__main__.HybridDeepNeuralNetwork'>, 'model_params': {'window_size': 30, 'input_size': 15}, 'scaler_cls': <class 'sklearn.preprocessing._data.MinMaxScaler'>, 'window_size': 30, 'stride': 1, 'batch_size': 512, 'initial_rul': 130, 'optimizer_cls': <class 'torch.optim.adam.Adam'>, 'lr': 0.001, 'num_epochs': 100, 'columns_to_drop': ['operational_setting_1', 'operational_setting_2', 'operational_setting_3', 'sensor_1', 'sensor_5', 'sensor_6', 'sensor_10', 'sensor_16', 'sensor_18', 'sensor_19']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1040.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Train Loss: 4047.322091238839\n",
      "Epoch 20/100, Train Loss: 2413.3984793526784\n",
      "Epoch 30/100, Train Loss: 1947.5848005022322\n",
      "Epoch 40/100, Train Loss: 1861.7769496372769\n",
      "Epoch 50/100, Train Loss: 1852.7801374162946\n",
      "Epoch 60/100, Train Loss: 1852.8918910435268\n",
      "Epoch 70/100, Train Loss: 1851.584783063616\n",
      "Epoch 80/100, Train Loss: 1853.7275425502232\n",
      "Epoch 90/100, Train Loss: 1853.2325439453125\n",
      "Epoch 100/100, Train Loss: 733.9235770089285\n",
      "Test Loss MSE: 1277.593505859375, RMSE: 35.74344003952858, Score: 238.30503845214844\n",
      "Final model saved at results/FD001_hdnn.pth\n",
      "Results saved at results/FD001_hdnn.json\n"
     ]
    }
   ],
   "source": [
    "columns_to_drop = ['operational_setting_1', 'operational_setting_2', 'operational_setting_3', \n",
    "               'sensor_1', 'sensor_5', 'sensor_6', 'sensor_10', \n",
    "               'sensor_16', 'sensor_18', 'sensor_19']\n",
    "param_grid_hdnn = {\n",
    "    \"model_cls\": HybridDeepNeuralNetwork,\n",
    "    \"model_params\": {\n",
    "            \"window_size\": 30,\n",
    "        },\n",
    "    \"scaler_cls\": MinMaxScaler,\n",
    "    \"window_size\": 30,\n",
    "    \"stride\": 1,\n",
    "    \"batch_size\": 512,\n",
    "    \"initial_rul\": 130,\n",
    "    \"optimizer_cls\": optim.Adam,\n",
    "    \"lr\": 0.001,\n",
    "    \"num_epochs\": 100,\n",
    "    \"columns_to_drop\": columns_to_drop\n",
    "}\n",
    "\n",
    "\n",
    "results = grid_search(\"FD001\", param_grid_hdnn, \"hdnn\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
