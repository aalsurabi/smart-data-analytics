{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ad642c71-642c-468d-9ac5-640d9e0b97a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import ray\n",
    "from ray import train, tune\n",
    "from ray.train import Checkpoint\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414f2260-0164-4a26-b02f-73f80da5167e",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3d6f1b98-2219-4767-a0a5-595839156464",
   "metadata": {},
   "outputs": [],
   "source": [
    "FD001_train = pd.read_csv('data/train_FD001.txt', sep='\\s+', header=None)\n",
    "FD001_test = pd.read_csv('data/test_FD001.txt', sep='\\s+', header=None)\n",
    "FD002_train = pd.read_csv('data/train_FD002.txt', sep='\\s+', header=None)\n",
    "FD002_test = pd.read_csv('data/test_FD002.txt', sep='\\s+', header=None)\n",
    "FD003_train = pd.read_csv('data/train_FD003.txt', sep='\\s+', header=None)\n",
    "FD003_test = pd.read_csv('data/test_FD003.txt', sep='\\s+', header=None)\n",
    "FD004_train = pd.read_csv('data/train_FD004.txt', sep='\\s+', header=None)\n",
    "FD004_test = pd.read_csv('data/test_FD004.txt', sep='\\s+', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "902d0635-b365-4807-a2fd-0b9720f1d6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "FD001_test_targets = pd.read_csv('data/RUL_FD001.txt',sep='\\s+', header=None, names=[\"RUL\"])\n",
    "FD002_test_targets = pd.read_csv('data/RUL_FD002.txt',sep='\\s+', header=None, names=[\"RUL\"])\n",
    "FD003_test_targets = pd.read_csv('data/RUL_FD003.txt',sep='\\s+', header=None, names=[\"RUL\"])\n",
    "FD004_test_targets = pd.read_csv('data/RUL_FD004.txt',sep='\\s+', header=None, names=[\"RUL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ae35e539-1ef6-43f7-a8da-9a467e827d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column names\n",
    "index_columns_names =  [\"unit_number\",\"cycle\"]\n",
    "operational_settings_columns_names = [\"operational_setting_\"+str(i) for i in range(1,4)]\n",
    "sensor_measure_columns_names =[\"sensor_\"+str(i) for i in range(1,22)]\n",
    "input_file_column_names = index_columns_names + operational_settings_columns_names + sensor_measure_columns_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "694e6bf1-1031-4d83-b1a0-5d4646c5fe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "FD001_train.columns = input_file_column_names\n",
    "FD001_test.columns = input_file_column_names\n",
    "FD002_train.columns = input_file_column_names\n",
    "FD002_test.columns = input_file_column_names\n",
    "FD003_train.columns = input_file_column_names\n",
    "FD003_test.columns = input_file_column_names\n",
    "FD004_train.columns = input_file_column_names\n",
    "FD004_test.columns = input_file_column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433e4614-4682-4f03-9a4b-e614dedfb29f",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a37dae11-cceb-43e1-ba81-316c4b33433f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns_to_drop = ['operational_setting_1', 'operational_setting_2', 'operational_setting_3', \n",
    "                   'sensor_1', 'sensor_5', 'sensor_6', 'sensor_7', \n",
    "                   'sensor_10', 'sensor_16', 'sensor_18', 'sensor_19']\n",
    "columns_to_scale = operational_settings_columns_names + sensor_measure_columns_names\n",
    "columns = [col for col in input_file_column_names if col not in columns_to_drop]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b6713357-1366-4310-9008-c860a20f34c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calculate_rul(df, initial_rul=0):\n",
    "    \"\"\"\n",
    "    Calculates target RUL for a dataframe. If initial_rul is != 0 piece-wise linear degradation is calculated \n",
    "    (Initially, RUL is set to constant value until degradation starts). Otherwise, RUL is linear degradation \n",
    "    and starts with max_cycle number for a motor unit.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame containing the data\n",
    "    - initial_rul: Initial constant RUL value before degradation starts\n",
    "    \n",
    "    Returns:\n",
    "    - numpy array containing the RUL values for the entire dataframe\n",
    "    \"\"\"\n",
    "    grouped = df.groupby(\"unit_number\")\n",
    "    ruls = []\n",
    "\n",
    "    for _, unit in grouped:\n",
    "        max_cycle = unit.shape[0]\n",
    "        targets = np.arange(max_cycle, -1, -1)[:-1]  # create array from max_cycle-1 to 0\n",
    "        if initial_rul > 0:\n",
    "            targets = np.clip(targets, None, initial_rul)\n",
    "        ruls.append(targets)\n",
    "    \n",
    "    return np.concatenate(ruls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bf080dd7-798a-4229-8d6d-42e30a34d62d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scale(df, scaler, train=True):\n",
    "    df_scaled = df.copy()\n",
    "    columns_to_scale = operational_settings_columns_names + sensor_measure_columns_names\n",
    "    if train: \n",
    "        df_scaled[columns_to_scale] = scaler.fit_transform(df_scaled[columns_to_scale])\n",
    "    else:\n",
    "        df_scaled[columns_to_scale] = scaler.transform(df_scaled[columns_to_scale])\n",
    "    return df_scaled, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1579c7be-1d1f-4906-9db7-c08298aa2c07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_sequence(df, window_size=30, stride=1):\n",
    "    grouped = df.groupby(\"unit_number\")\n",
    "    X_processed = []\n",
    "    y_processed = []\n",
    "    for _, unit in grouped:\n",
    "        unit_data = unit.sort_values(by=\"cycle\", ascending=True)\n",
    "        target = unit_data[\"rul\"]\n",
    "        windows = extract_windows(unit_data.drop([\"cycle\", \"rul\"],axis=1), window_size, stride)\n",
    "        X_processed.append(windows)\n",
    "        targets_for_windows = target[-windows.shape[0]::] #the target for a window is target rul of value at the end of window. So the first num_windows values of target care cut off\n",
    "        y_processed.append(targets_for_windows)\n",
    "    X_processed = np.concatenate(X_processed)\n",
    "    y_processed = np.concatenate(y_processed)\n",
    "    return X_processed, y_processed\n",
    "    \n",
    "def extract_windows(data, window_size, stride):\n",
    "    if data.shape[0] < window_size:\n",
    "            raise AssertionError(\"Window length is larger than sequence length \")\n",
    "    windows = sliding_window_view(data, window_shape=(window_size, data.shape[1])).squeeze() #squeeze to remove dimension with 1\n",
    "    if stride != 1:\n",
    "        windows = windows[::self.stride]\n",
    "    return windows  \n",
    "\n",
    "\n",
    "# Take last n windows for final prediction\n",
    "def generate_test_sequence(df, n_windows= 1, window_size=30, stride=1):\n",
    "    grouped = df.groupby(\"unit_number\")\n",
    "    X_processed = []\n",
    "    for _, unit in grouped:\n",
    "        unit_data = unit.sort_values(by=\"cycle\", ascending=True)\n",
    "        windows = extract_windows(unit_data.drop([\"cycle\"],axis=1), window_size, stride)\n",
    "        windows = windows[-n_windows:]  # take only last n windows\n",
    "        X_processed.append(windows)\n",
    "\n",
    "    if n_windows==1:\n",
    "        X_processed = np.concatenate(X_processed) #shape is (units, features)\n",
    "    #otherwise return list with length units, each element has n_windows which corresponds to last n windows of that unit\n",
    "\n",
    "    return X_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df209baf-6197-42c5-b002-92e67b73b4d6",
   "metadata": {},
   "source": [
    "## Testing preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a6dcebca-e49d-4e66-9955-5e6484d4c807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rul piecewise-linear:  [130 130 130 130 130 130 130 130 130 130 130 130 130 130 130 130 130 130\n",
      " 130 130 130 130 130 130 130 130 130 130 130 130 130 130 130 130 130 130\n",
      " 130 130 130 130 130 130 130 130 130 130 130 130 130 130 130 130 130 130\n",
      " 130 130 130 130 130 130 130 130 130 129 128 127 126 125 124 123 122 121\n",
      " 120 119 118 117 116 115 114 113 112 111 110 109 108 107 106 105 104 103\n",
      " 102 101 100  99  98  97  96  95  94  93  92  91  90  89  88  87  86  85\n",
      "  84  83  82  81  80  79  78  77  76  75  74  73  72  71  70  69  68  67\n",
      "  66  65  64  63  62  61  60  59  58  57  56  55  54  53  52  51  50  49\n",
      "  48  47  46  45  44  43  42  41  40  39  38  37  36  35  34  33  32  31\n",
      "  30  29  28  27  26  25  24  23  22  21  20  19  18  17  16  15  14  13\n",
      "  12  11  10   9   8   7   6   5   4   3   2   1]\n",
      "rul linear:  [192 191 190 189 188 187 186 185 184 183 182 181 180 179 178 177 176 175\n",
      " 174 173 172 171 170 169 168 167 166 165 164 163 162 161 160 159 158 157\n",
      " 156 155 154 153 152 151 150 149 148 147 146 145 144 143 142 141 140 139\n",
      " 138 137 136 135 134 133 132 131 130 129 128 127 126 125 124 123 122 121\n",
      " 120 119 118 117 116 115 114 113 112 111 110 109 108 107 106 105 104 103\n",
      " 102 101 100  99  98  97  96  95  94  93  92  91  90  89  88  87  86  85\n",
      "  84  83  82  81  80  79  78  77  76  75  74  73  72  71  70  69  68  67\n",
      "  66  65  64  63  62  61  60  59  58  57  56  55  54  53  52  51  50  49\n",
      "  48  47  46  45  44  43  42  41  40  39  38  37  36  35  34  33  32  31\n",
      "  30  29  28  27  26  25  24  23  22  21  20  19  18  17  16  15  14  13\n",
      "  12  11  10   9   8   7   6   5   4   3   2   1]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "unit_1_df = FD001_train[FD001_train[\"unit_number\"]==1].copy()\n",
    "print(\"rul piecewise-linear: \" , calculate_rul(unit_1_df, initial_rul=130))\n",
    "print(\"rul linear: \",  calculate_rul(unit_1_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c7b3bb7a-2eb5-401d-a2fa-490f642a5a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2]\n",
      " [ 3  4]\n",
      " [ 5  6]\n",
      " [ 7  8]\n",
      " [ 9 10]\n",
      " [11 12]\n",
      " [13 14]\n",
      " [15 16]\n",
      " [17 18]\n",
      " [19 20]]\n",
      "(10, 2)\n",
      "Extracted windows \n",
      " [[[ 1  2]\n",
      "  [ 3  4]]\n",
      "\n",
      " [[ 3  4]\n",
      "  [ 5  6]]\n",
      "\n",
      " [[ 5  6]\n",
      "  [ 7  8]]\n",
      "\n",
      " [[ 7  8]\n",
      "  [ 9 10]]\n",
      "\n",
      " [[ 9 10]\n",
      "  [11 12]]\n",
      "\n",
      " [[11 12]\n",
      "  [13 14]]\n",
      "\n",
      " [[13 14]\n",
      "  [15 16]]\n",
      "\n",
      " [[15 16]\n",
      "  [17 18]]\n",
      "\n",
      " [[17 18]\n",
      "  [19 20]]]\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(1,21).reshape(10,2) #first dimension is time step\n",
    "print(a)\n",
    "print(a.shape)\n",
    "b = np.arange(1,11)\n",
    "X = extract_windows(a, window_size=2, stride=1)\n",
    "print(\"Extracted windows \\n\", X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ccd13dc7-ef68-4e2a-ae06-e175b98f0515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20631, 27)\n",
      "(20631,)\n",
      "(20631, 5)\n",
      "   unit_number  cycle  sensor_2  sensor_17  rul\n",
      "0            1      1 -1.721725  -0.781710  192\n",
      "1            1      2 -1.061780  -0.781710  191\n",
      "2            1      3 -0.661813  -2.073094  190\n",
      "3            1      4 -0.661813  -0.781710  189\n",
      "4            1      5 -0.621816  -0.136018  188\n"
     ]
    }
   ],
   "source": [
    "data = FD001_train.copy()\n",
    "\n",
    "X= data\n",
    "X[\"rul\"] = calculate_rul(data)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "X, _ = scale(X, StandardScaler())\n",
    "columns_to_drop_test = ['operational_setting_1', 'operational_setting_2', 'operational_setting_3', \n",
    "                   'sensor_1', 'sensor_3', 'sensor_4', 'sensor_5', 'sensor_6', 'sensor_7', 'sensor_8', 'sensor_9',\n",
    "                   'sensor_10', 'sensor_11', 'sensor_12', 'sensor_13', 'sensor_14', 'sensor_15', 'sensor_16', 'sensor_18', 'sensor_19', 'sensor_20', 'sensor_21']\n",
    "X = X.drop( columns_to_drop_test, axis=1)\n",
    "print(X.shape)\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a50dd1fc-b7de-4360-9263-e76ee1b35249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17731, 30, 4)\n",
      "(17731,)\n"
     ]
    }
   ],
   "source": [
    "X_seq, y_seq = generate_sequence(X, window_size=30, stride=1)\n",
    "print(X_seq.shape)\n",
    "print(y_seq.shape)\n",
    "#for i,j  in zip(X_seq[:400], y_seq[:400]):\n",
    "#    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "40d151c6-b1d4-4bd6-80e8-f5af0adaed69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13096, 26)\n",
      "(100, 30, 25)\n",
      "100\n",
      "(5, 30, 25)\n"
     ]
    }
   ],
   "source": [
    "data = FD001_test.copy()\n",
    "X_seq = generate_test_sequence(data, window_size=30, stride=1)\n",
    "print(data.shape)\n",
    "print(X_seq.shape)\n",
    "X_seq_n = generate_test_sequence(data, n_windows=5, window_size=30, stride=1)\n",
    "print(len(X_seq_n))\n",
    "print(X_seq_n[3].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e693c9c7-a947-445f-ab7b-d929492f6e36",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "56448cbb-7c42-4c19-a0d2-70d43f482828",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, output_size=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out.squeeze()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df4e0b0c-5d8d-4953-9cbc-dcf41fd4d901",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skorch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NeuralNetRegressor\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GridSearchCV\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GroupKFold\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'skorch'"
     ]
    }
   ],
   "source": [
    "from skorch import NeuralNetRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "\n",
    "# Define RMSE as the evaluation metric\n",
    "def neg_rmse(y_true, y_pred):\n",
    "    return - np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# Make RMSE scorer\n",
    "rmse_scorer = make_scorer(neg_rmse, greater_is_better=True)\n",
    "\n",
    "\n",
    "X = FD001_train.copy()\n",
    "y = calculate_rul(X, initial_rul=130)\n",
    "\n",
    "X, _  = scale(X, scaler=StandardScaler(), train=True)\n",
    "X = X.drop(columns_to_drop, axis=1)\n",
    "\n",
    "X, y = generate_sequence(X,y, window_size=30, stride=1)\n",
    "X = torch.tensor(X, dtype=torch.float)\n",
    "y = torch.tensor(y, dtype=torch.float)\n",
    "print(\"X shape\", X.shape)\n",
    "print(\"Y Shape\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd98caae-37ba-4443-af7e-b4165212cf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model = NeuralNetRegressor(\n",
    "    module=LSTMModel,\n",
    "    criterion=RMSELoss,\n",
    "    optimizer=optim.Adam,\n",
    "    max_epochs=3,\n",
    "    module__hidden_size=64,\n",
    "    module__input_size=14,\n",
    "    module__num_layers=2,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "simple_model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6eccde-169c-477e-ad63-488b2b69d11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = NeuralNetRegressor(\n",
    "    module=LSTMModel,\n",
    "    criterion=RMSELoss,\n",
    "    optimizer=optim.Adam,\n",
    "    max_epochs=3,\n",
    "    batch_size=32,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "\n",
    "# Define the parameter grid to search over\n",
    "param_grid = {\n",
    "    'module__hidden_size': [64],  # Hidden dimension of LSTM\n",
    "    'module__num_layers': [ 2],  # Number of LSTM layers\n",
    "    'module__input_size' : [14] # Number of features\n",
    "}\n",
    "\n",
    "group_kfold = GroupKFold(n_splits=5)\n",
    "groups = X[:,0,0]\n",
    "print(X.shape)\n",
    "print(groups.shape)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring=rmse_scorer, cv=group_kfold)\n",
    "grid_search.fit(X, y, groups=groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc29bb6b-4222-4435-9e76-6530e608f9ca",
   "metadata": {},
   "source": [
    "# Grid Search with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4a9371ab-81de-434f-95ff-684158cd97d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CMAPPSDataset(Dataset):\n",
    "    def __init__(self, data, scaler, columns_to_drop, window_size, stride, initial_rul = 0, train=True):\n",
    "        self.data = data\n",
    "        if scaler is None:\n",
    "            self.scaler = StandardScaler()\n",
    "        elif isinstance(scaler, type):\n",
    "            self.scaler = scaler()  # scaler is a class, so we instantiate it\n",
    "        else:\n",
    "            self.scaler = scaler  # scaler is already an instance\n",
    "        \n",
    "        self.data[\"rul\"] = calculate_rul(self.data, initial_rul = initial_rul)\n",
    "\n",
    "        # if train is true scaler does fit_transform(). Else only transform()\n",
    "        self.X, self.scaler  = scale(self.data, scaler=self.scaler, train=train)\n",
    "        self.X = self.X.drop(columns_to_drop,axis=1)\n",
    "        self.X_seq, self.y_seq = generate_sequence(self.X, window_size=window_size, stride=stride)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_seq)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "         sample =  torch.tensor(self.X_seq[idx], dtype=torch.float)\n",
    "         target =  torch.tensor(self.y_seq[idx], dtype=torch.float)\n",
    "         return sample, target\n",
    "\n",
    "    def get_scaler(self):\n",
    "        return self.scaler\n",
    "\n",
    "\n",
    "class CMAPPSTestDataset(Dataset):\n",
    "    def __init__(self, data, targets, scaler, columns_to_drop, window_size, stride, n_windows=1):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.scaler = scaler #is already instantiated scaler\n",
    "        \n",
    "        self.y = targets.squeeze()\n",
    "\n",
    "        # if train is true scaler is fit and transform. Else only transform\n",
    "        self.X, self.scaler  = scale(self.data, scaler=self.scaler, train=train)\n",
    "        self.X = self.X.drop(columns_to_drop,axis=1)\n",
    "        self.X = self.X\n",
    "        self.X_seq = generate_test_sequence(self.X, n_windows=1, window_size=window_size, stride=stride)\n",
    "        \n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_seq)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "         sample =  torch.tensor(self.X_seq[idx], dtype=torch.float)\n",
    "         target =  torch.tensor(self.y[idx], dtype=torch.float)\n",
    "         return sample, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c948949f-e91d-4e79-bcbc-db2a78c86914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_s_score(rul_true, rul_pred):\n",
    "    \"\"\"\n",
    "    Both rul_true and rul_pred should be 1D torch tensors.\n",
    "    \"\"\"\n",
    "    diff = rul_pred - rul_true\n",
    "    return torch.sum(torch.where(diff < 0, torch.exp(-diff/13)-1, torch.exp(diff/10)-1))\n",
    "\n",
    "def reset_weights(m):\n",
    "  '''\n",
    "    Try resetting model weights to avoid\n",
    "    weight leakage.\n",
    "  '''\n",
    "  for layer in m.children():\n",
    "   if hasattr(layer, 'reset_parameters'):\n",
    "    #print(f'Reset trainable parameters of layer = {layer}')\n",
    "    layer.reset_parameters()\n",
    "       \n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs).squeeze()\n",
    "        if outputs.dim() == 0:  # Handle the case where outputs is a scalar\n",
    "            outputs = outputs.unsqueeze(0)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        running_loss +=loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    total_train_loss = running_loss / len(train_loader)\n",
    "    #print(f\"Epoch {epoch+1}, Training Loss: {np.mean(total_loss}\")\n",
    "    return total_train_loss\n",
    "\n",
    "\n",
    "def evaluate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    scores = []\n",
    "    #total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs).squeeze()\n",
    "            if outputs.dim() == 0:  # Handle the case where outputs is a scalar\n",
    "                outputs = outputs.unsqueeze(0)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss +=loss.item()  * inputs.size(0)\n",
    "            score = compute_s_score(outputs, targets)\n",
    "            scores.append(score.item())\n",
    "\n",
    "    total_loss = running_loss / len(val_loader.dataset)\n",
    "    mean_score = np.mean(scores)\n",
    "    return total_loss, mean_score # Divide by the total number of samples\n",
    "\n",
    "def train_model(params: dict, data):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.manual_seed(42)\n",
    "    #train_val_split = GroupShuffleSplit(n_splits=2, train_size=.8, random_state=42)\n",
    "    #train_inds, val_inds = train_val_split.split(X,y, data[\"unit_number\"])\n",
    "    #print(len(train_inds))\n",
    "\n",
    "    #X_train, y_train = X.iloc[train_inds], y.iloc[train_inds]\n",
    "    #X_val, y_val = X.iloc[val_inds], y.iloc[val_inds]\n",
    "\n",
    "\n",
    "    # Cross Validation\n",
    "    n_splits=5\n",
    "    losses = []\n",
    "    scores = []\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    \n",
    "    for fold, (train_ids, val_ids) in enumerate(GroupKFold(n_splits=n_splits).split(data, y=None, groups= data[\"unit_number\"].copy())):\n",
    "\n",
    "        # Load datasets and perform preproccesing\n",
    "        train_subset = CMAPPSDataset( data=data.iloc[train_ids], scaler=params[\"scaler_cls\"], columns_to_drop=params[\"columns_to_drop\"], window_size=params[\"window_size\"], stride=params[\"stride\"], initial_rul = params[\"initial_rul\"], train=True)\n",
    "        train_scaler = train_subset.get_scaler()\n",
    "        \n",
    "        val_subset = CMAPPSDataset( data=data.iloc[val_ids], scaler=train_scaler, columns_to_drop=params[\"columns_to_drop\"], window_size=params[\"window_size\"], stride=params[\"stride\"],initial_rul =  params[\"initial_rul\"], train=False)\n",
    "        \n",
    "\n",
    "        train_loader = DataLoader(train_subset, batch_size=params[\"batch_size\"], shuffle=True, num_workers=8)\n",
    "        val_loader = DataLoader(val_subset, batch_size=params[\"batch_size\"], shuffle=True, num_workers=8)\n",
    "\n",
    "        # Initialize model\n",
    "        input_sample, _ = train_subset[0]\n",
    "        input_size = input_sample.shape[-1]\n",
    "        model = params[\"model_cls\"](input_size=input_size, hidden_size=params[\"hidden_size\"], num_layers=params[\"num_layers\"], output_size=1).to(device)\n",
    "        model.apply(reset_weights)\n",
    "\n",
    "        #Initialize optimizer\n",
    "        optimizer = params[\"optimizer_cls\"](params = model.parameters(),lr=params[\"lr\"])\n",
    "\n",
    "\n",
    "        # Train loop\n",
    "        for epoch in range(params[\"num_epochs\"] ):\n",
    "            train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "            #print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss}\")\n",
    "\n",
    "        # Evaluate\n",
    "        val_loss, score = evaluate(model, val_loader, criterion, device)\n",
    "        #print(f\"Fold {fold + 1}/{n_splits}, Validation Loss  MSE: {val_loss}, RMSE: {np.sqrt(val_loss)}\")\n",
    "        losses.append(val_loss)\n",
    "        scores.append(score)\n",
    "\n",
    "    \n",
    "    avg_mse_loss = np.mean(losses)\n",
    "    rmse_loss = np.sqrt(avg_mse_loss)\n",
    "    avg_score =  np.mean(scores)\n",
    "    #print(f'Average Loss over all folds:  MSE {avg_mse_loss}, RMSE {rmse_loss}')\n",
    "    train.report({\"mse\": avg_mse_loss, 'rmse': rmse_loss, 'score': avg_score })\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1fb4743f-4dbb-4055-ad29-6a49ec10ed79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(train_data,  param_grid):\n",
    "    scheduler = ASHAScheduler(\n",
    "        max_t=10,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "\n",
    "    tuner = tune.Tuner(\n",
    "        tune.with_resources(\n",
    "            tune.with_parameters(train_model, data=train_data), \n",
    "         resources={\"cpu\":8, \"gpu\": 1}),\n",
    "        tune_config=tune.TuneConfig(\n",
    "                metric=\"rmse\",\n",
    "                mode=\"min\",\n",
    "                scheduler=scheduler,\n",
    "                num_samples=1,\n",
    "            ),\n",
    "     param_space=param_grid)\n",
    "    results = tuner.fit()\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "467277d0-dc25-473f-83e1-6cacaceadda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_train_and_evaluate(params, train_data, test_data, targets ,save_dir=\"models\", name=\"final_model\"):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "   \n",
    "    # Parameters for training\n",
    "    train_dataset = CMAPPSDataset(data=train_data, scaler=params[\"scaler_cls\"], columns_to_drop=params[\"columns_to_drop\"], window_size=params[\"window_size\"], stride=params[\"stride\"], initial_rul=params[\"initial_rul\"], train=True)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True, num_workers=8)\n",
    "    scaler = train_dataset.get_scaler()\n",
    "\n",
    "    test_dataset = CMAPPSTestDataset(data=test_data, n_windows=1, targets=targets, scaler=scaler, columns_to_drop=params[\"columns_to_drop\"] , window_size=params[\"window_size\"],stride=params[\"stride\"])\n",
    "    test_loader = DataLoader(test_dataset, batch_size=params[\"batch_size\"], shuffle=False, num_workers=8)\n",
    "    \n",
    "    # Instantiate model\n",
    "    input_sample, _ = train_dataset[0]\n",
    "    test_sample,_ = test_dataset[0]\n",
    "    print(test_sample.shape)\n",
    "    input_size = input_sample.shape[-1]\n",
    "    model = params[\"model_cls\"](input_size=input_size, hidden_size=params[\"hidden_size\"], num_layers=params[\"num_layers\"], output_size=1).to(device)\n",
    "    model.apply(reset_weights)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = params[\"optimizer_cls\"](model.parameters(), lr=params[\"lr\"])   \n",
    "    # Train loop\n",
    "    for epoch in range(params[\"num_epochs\"]):\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{params['num_epochs']}, Train Loss: {train_loss}\")\n",
    "    \n",
    "     # Evaluate\n",
    "    test_mse_loss, test_score = evaluate(model, test_loader, criterion, device)\n",
    "    test_rmse_loss = np.sqrt(test_mse_loss)\n",
    "    print(f'Test Loss MSE: {test_mse_loss}, RMSE: {test_rmse_loss}, Score: {test_score}')\n",
    "\n",
    "    # Save final model\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    final_model_path = os.path.join(save_dir, name + '.pth')\n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "    print(f\"Final model saved at {final_model_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2fd29b8f-bbe7-4c19-a0ed-15813552639e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-05-29 12:39:02</td></tr>\n",
       "<tr><td>Running for: </td><td>00:01:02.99        </td></tr>\n",
       "<tr><td>Memory:      </td><td>13.3/503.0 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=0<br>Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -37.91897192213916<br>Logical resource usage: 8.0/8 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:A100)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status    </th><th>loc               </th><th style=\"text-align: right;\">  hidden_size</th><th style=\"text-align: right;\">  stride</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    mse</th><th style=\"text-align: right;\">  rmse</th><th style=\"text-align: right;\">      score</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_480b6_00000</td><td>TERMINATED</td><td>10.38.128.5:543667</td><td style=\"text-align: right;\">           32</td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         60.2664</td><td style=\"text-align: right;\">1437.85</td><td style=\"text-align: right;\">37.919</td><td style=\"text-align: right;\">7.29789e+07</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-29 12:39:02,157\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n",
      "2024-05-29 12:39:02,160\tINFO tune.py:1007 -- Wrote the latest version of all result files and experiment state to '/home/jovyan/ray_results/train_model_2024-05-29_12-37-59' in 0.0086s.\n",
      "2024-05-29 12:39:02,163\tINFO tune.py:1039 -- Total run time: 63.01 seconds (62.98 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    \"model_cls\": LSTMModel,\n",
    "    \"scaler_cls\": StandardScaler,\n",
    "    \"window_size\" : 30,\n",
    "    \"stride\" : tune.grid_search([1]),\n",
    "    \"batch_size\" : 32,\n",
    "    \"initial_rul\": 0,\n",
    "    \"hidden_size\": tune.grid_search([ 32]),\n",
    "    \"num_layers\": 2,\n",
    "    \"optimizer_cls\": optim.Adam,\n",
    "    \"lr\": 0.001,\n",
    "    \"num_epochs\": 10,\n",
    "    \"columns_to_drop\": columns_to_drop\n",
    "}\n",
    "\n",
    "\n",
    "train_data = FD001_train.copy()\n",
    "results = grid_search(train_data, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a98deb00-ff9c-4a87-bfed-dd1d09170aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial config: {'model_cls': <class '__main__.LSTMModel'>, 'scaler_cls': <class 'sklearn.preprocessing._data.StandardScaler'>, 'window_size': 30, 'stride': 1, 'batch_size': 32, 'initial_rul': 0, 'hidden_size': 32, 'num_layers': 2, 'optimizer_cls': <class 'torch.optim.adam.Adam'>, 'lr': 0.001, 'num_epochs': 10, 'columns_to_drop': ['operational_setting_1', 'operational_setting_2', 'operational_setting_3', 'sensor_1', 'sensor_5', 'sensor_6', 'sensor_7', 'sensor_10', 'sensor_16', 'sensor_18', 'sensor_19']}\n",
      "---------------\n",
      "Best trial final validation loss: 37.91897192213916\n"
     ]
    }
   ],
   "source": [
    "best_result = results.get_best_result(\"rmse\")\n",
    "best_params = best_result.config\n",
    "print(\"Best trial config: {}\".format(best_params))\n",
    "print(\"---------------\")\n",
    "print(\"Best trial final validation loss: {}\".format(\n",
    "        best_result.metrics[\"rmse\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ea6c4ad7-6be2-40af-8ddd-fd66e88901e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 15])\n",
      "torch.Size([30, 14])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[124], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m test_data \u001b[38;5;241m=\u001b[39m FD001_test\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      3\u001b[0m targets \u001b[38;5;241m=\u001b[39m FD001_test_targets\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m----> 4\u001b[0m \u001b[43mfinal_train_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfinal_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[123], line 25\u001b[0m, in \u001b[0;36mfinal_train_and_evaluate\u001b[0;34m(params, train_data, test_data, targets, save_dir, name)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Train loop\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m---> 25\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[108], line 21\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     20\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 21\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1295\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1295\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1296\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1297\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/multiprocessing/queues.py:122\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rlock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# unserialize the data after having released the lock\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _ForkingPickler\u001b[38;5;241m.\u001b[39mloads(res)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/multiprocessing/reductions.py:111\u001b[0m, in \u001b[0;36mrebuild_tensor\u001b[0;34m(cls, storage, metadata)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrebuild_tensor\u001b[39m(\u001b[38;5;28mcls\u001b[39m, storage, metadata):\n\u001b[1;32m    110\u001b[0m     storage_offset, size, stride, requires_grad \u001b[38;5;241m=\u001b[39m metadata\n\u001b[0;32m--> 111\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rebuild_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mparameter\u001b[38;5;241m.\u001b[39mParameter:\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;66;03m# we have to pass requires_grad into constructor, rather than set it as an\u001b[39;00m\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;66;03m# attribute later, because it's an important check for Integer Tensors to\u001b[39;00m\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;66;03m# have requires_grad=False (or else they raise an error)\u001b[39;00m\n\u001b[1;32m    116\u001b[0m         t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mparameter\u001b[38;5;241m.\u001b[39mParameter(t, requires_grad\u001b[38;5;241m=\u001b[39mrequires_grad)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/_utils.py:181\u001b[0m, in \u001b[0;36m_rebuild_tensor\u001b[0;34m(storage, storage_offset, size, stride)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_rebuild_tensor\u001b[39m(storage, storage_offset, size, stride):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;66;03m# first construct a tensor with the correct dtype/device\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_untyped_storage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mset_(storage\u001b[38;5;241m.\u001b[39m_untyped_storage, storage_offset, size, stride)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_data = FD001_train.copy()\n",
    "test_data = FD001_test.copy()\n",
    "targets = FD001_test_targets.copy()\n",
    "final_train_and_evaluate(best_params, train_data, test_data, targets , name=\"final_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9551093d-d2c5-4dfa-a4d8-e79f48e75a31",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb8b529-973a-4247-acec-b783a67a5cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = FD001_train.copy()\n",
    "\n",
    "avg_loss, scaler, model = final_train(best_params,  train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6910f579-a8ae-4bc2-9309-a9d8693ccae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "test_data = FD001_test.copy()\n",
    "test_targets = FD001_test_targets.copy()\n",
    "test_dataset = CMAPPSTestDataset(data=data, n_windows=1, targets=test_targets, scaler=scaler, columns_to_drop=best_params[\"columns_to_drop\"] , window_size=best_params[\"window_size\"],stride=best_params[\"stride\"])\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "final_loss = evaluate(model, test_loader, RMSELoss(), device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "print(final_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee03bd7c-9f8d-4f9d-857e-9952c9b023eb",
   "metadata": {},
   "source": [
    "## With piece-wise RUL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682fbb8e-ba07-4cc2-895d-5d9b2d1787ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"model_cls\": [LSTMModel],\n",
    "    \"input_size\": [14],\n",
    "    \"scaler_cls\": [StandardScaler],\n",
    "    \"window_size\" : [30],\n",
    "    \"stride\" : [1],\n",
    "    \"batch_size\" : [32],\n",
    "    \"initial_rul\": [130], # set initial RUL to 130 for piece-wise linear RUL\n",
    "    \"hidden_size\": [64],\n",
    "    \"num_layers\": [2],\n",
    "    \"criterion_cls\": [RMSELoss],\n",
    "    \"optimizer_cls\": [optim.Adam],\n",
    "    \"lr\": [0.001],\n",
    "    \"num_epochs\": [10],\n",
    "    \"columns_to_drop\": [columns_to_drop]\n",
    "}\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "param_combinations = list(itertools.product(*param_grid.values()))\n",
    "train_data = FD001_train.copy()\n",
    "\n",
    "# Perform grid search\n",
    "best_loss = float('inf')\n",
    "best_params = None\n",
    "for params in param_combinations:\n",
    "    params_dict = {param_name: param_value for param_name, param_value in zip(param_grid.keys(), params)}\n",
    "    print(f'Training with hyperparameters: {params_dict}')\n",
    "\n",
    "    # Call train function with current hyperparameters\n",
    "    avg_loss = train(params_dict, train_data)  # Assuming data is defined elsewhere\n",
    "\n",
    "    # Check if current hyperparameters lead to best loss\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        best_params = params_dict\n",
    "\n",
    "print(f'Best hyperparameters: {best_params}, Best Loss: {best_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f431b7a-724b-491f-8e6d-e52f13afecfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = FD001_train.copy()\n",
    "avg_loss, scaler, model = final_train(best_params,  train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ad9bc0-5724-4e90-88af-0add012067dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "test_data = FD001_test.copy()\n",
    "test_targets = FD001_test_targets.copy()\n",
    "test_dataset = CMAPPSTestDataset(data=data, n_windows=1, targets=test_targets, scaler=scaler, columns_to_drop=best_params[\"columns_to_drop\"] , window_size=best_params[\"window_size\"],stride=best_params[\"stride\"])\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "final_loss = evaluate(model, test_loader, RMSELoss(), device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "print(final_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0222418-121f-45b0-9499-b68fe03edf56",
   "metadata": {},
   "source": [
    "## Old Code"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee369c9b-350c-4341-8f78-21ae5bed1668",
   "metadata": {},
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class Preprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns_to_drop=None, scaler=None):\n",
    "        self.columns_to_drop = columns_to_drop\n",
    "        if scaler is None:\n",
    "            self.scaler = StandardScaler()\n",
    "        elif isinstance(scaler, type):\n",
    "            self.scaler = scaler()  # scaler is a class, so we instantiate it\n",
    "        else:\n",
    "            self.scaler = scaler  # scaler is already an instance\n",
    "        self.columns_to_scale = operational_settings_columns_names + sensor_measure_columns_names\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = X.copy()\n",
    "        X_transformed[self.columns_to_scale] =  self.scaler.transform(X[self.columns_to_scale])\n",
    "        X_transformed = X_transformed.drop(columns=self.columns_to_drop, axis=1)\n",
    "\n",
    "        return X_transformed\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        X_transformed = X.copy()\n",
    "        X_transformed[self.columns_to_scale] =  self.scaler.fit_transform(X[self.columns_to_scale])\n",
    "        X_transformed = X_transformed.drop(columns=self.columns_to_drop,axis=1)\n",
    "\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "raw",
   "id": "02ba8dfb-1e84-4bb1-a9ae-fdaeebc401d7",
   "metadata": {},
   "source": [
    "from skorch import NeuralNetRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "\n",
    "# Define RMSE as the evaluation metric\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# Make RMSE scorer\n",
    "rmse_scorer = make_scorer(rmse, greater_is_better=False)\n",
    "\n",
    "model = NeuralNetRegressor(\n",
    "    module=LSTMModel,\n",
    "    criterion=RMSELoss,\n",
    "    optimizer=optim.Adam,\n",
    "    max_epochs=10,\n",
    "    batch_size=32,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "preprocessor = Preprocessor(scaler=StandardScaler(), columns_to_drop=columns_to_drop)\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "# Define the parameter grid to search over\n",
    "param_grid = {\n",
    "    'model__module__hidden_size': [64],  # Hidden dimension of LSTM\n",
    "    'model__module__num_layers': [ 2],  # Number of LSTM layers\n",
    "    'model__module__input_size' : [14] # Number of features\n",
    "}\n",
    "\n",
    "\n",
    "X= FD001_train.copy()\n",
    "y = calculate_rul(X, initial_rul=130)\n",
    "\n",
    "# Apply preprocessing and sequence generation\n",
    "\n",
    "\n",
    "print(len(X))\n",
    "print(len(y))\n",
    "group_kfold = GroupKFold(n_splits=5)\n",
    "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, scoring=rmse_scorer, cv=group_kfold, n_jobs=-1)\n",
    "grid_search.fit(X, y, groups=X[\"unit_number\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
